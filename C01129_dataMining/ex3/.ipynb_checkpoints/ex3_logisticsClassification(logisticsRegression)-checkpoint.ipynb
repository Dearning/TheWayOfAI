{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0883cffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from numpy import * ;\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.linalg\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import scipy.optimize as opt\n",
    "\n",
    "# print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc46a543",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_csv(path):\n",
    "    data_read = pd.read_csv(path)\n",
    "    print(data_read.describe()) #查看数据特征\n",
    "    list = data_read.values.tolist()\n",
    "    data = np.array(list)\n",
    "    print(data.shape)\n",
    "    # print(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "870402e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          region       tenure          age      marital      address  \\\n",
      "count  1000.0000  1000.000000  1000.000000  1000.000000  1000.000000   \n",
      "mean      2.0220    35.526000    41.684000     0.495000    11.551000   \n",
      "std       0.8162    21.359812    12.558816     0.500225    10.086681   \n",
      "min       1.0000     1.000000    18.000000     0.000000     0.000000   \n",
      "25%       1.0000    17.000000    32.000000     0.000000     3.000000   \n",
      "50%       2.0000    34.000000    40.000000     0.000000     9.000000   \n",
      "75%       3.0000    54.000000    51.000000     1.000000    18.000000   \n",
      "max       3.0000    72.000000    77.000000     1.000000    55.000000   \n",
      "\n",
      "            income           ed       employ       retire       gender  ...  \\\n",
      "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000  ...   \n",
      "mean     77.535000     2.671000    10.987000     0.047000     0.517000  ...   \n",
      "std     107.044165     1.222397    10.082087     0.211745     0.499961  ...   \n",
      "min       9.000000     1.000000     0.000000     0.000000     0.000000  ...   \n",
      "25%      29.000000     2.000000     3.000000     0.000000     0.000000  ...   \n",
      "50%      47.000000     3.000000     8.000000     0.000000     1.000000  ...   \n",
      "75%      83.000000     4.000000    17.000000     0.000000     1.000000  ...   \n",
      "max    1668.000000     5.000000    47.000000     1.000000     1.000000  ...   \n",
      "\n",
      "            confer        ebill      loglong     logtoll     logequi  \\\n",
      "count  1000.000000  1000.000000  1000.000000  475.000000  386.000000   \n",
      "mean      0.502000     0.371000     2.182110    3.239705    3.568092   \n",
      "std       0.500246     0.483314     0.734552    0.413813    0.277556   \n",
      "min       0.000000     0.000000    -0.105361    1.749200    2.734368   \n",
      "25%       0.000000     0.000000     1.648659    2.970414    3.367726   \n",
      "50%       1.000000     0.000000     2.142999    3.208825    3.572344   \n",
      "75%       1.000000     1.000000     2.668095    3.488903    3.757414   \n",
      "max       1.000000     1.000000     4.604670    5.153292    4.352855   \n",
      "\n",
      "          logcard     logwire        lninc      custcat        churn  \n",
      "count  678.000000  296.000000  1000.000000  1000.000000  1000.000000  \n",
      "mean     2.854209    3.598296     3.957203     2.487000     0.274000  \n",
      "std      0.557286    0.367286     0.803754     1.120306     0.446232  \n",
      "min      1.011601    2.701361     2.197225     1.000000     0.000000  \n",
      "25%      2.463853    3.333543     3.367296     1.000000     0.000000  \n",
      "50%      2.847812    3.594569     3.850148     3.000000     0.000000  \n",
      "75%      3.208825    3.861780     4.418841     3.000000     1.000000  \n",
      "max      4.693639    4.718052     7.419381     4.000000     1.000000  \n",
      "\n",
      "[8 rows x 42 columns]\n",
      "(1000, 42)\n",
      "[  3.        11.        33.         1.         7.       136.\n",
      "   5.         5.         0.         0.         6.         1.\n",
      "   0.         1.         1.         4.4       20.75       0.\n",
      "  15.25      35.7       42.       211.45       0.       125.\n",
      " 380.35       0.         1.         1.         0.         1.\n",
      "   1.         1.         1.         0.         1.481605   3.032546\n",
      "        nan   2.72458    3.575151   4.912655   4.         1.      ]\n"
     ]
    }
   ],
   "source": [
    "file = \"telco.csv\"\n",
    "telco = load_csv(file)\n",
    "print(telco[1])\n",
    "# print(telco.description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d994b62",
   "metadata": {},
   "source": [
    "缺失值处理方法, 这里采用 删除https://cloud.tencent.com/developer/article/1680427"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a6110f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.         3.        11.        33.         1.         7.\n",
      " 136.         5.         5.         0.         0.         6.\n",
      "   1.         0.         1.         1.         4.4       20.75\n",
      "   0.        15.25      35.7       42.       211.45       0.\n",
      " 125.       380.35       0.         1.         1.         0.\n",
      "   1.         1.         1.         1.         0.         1.481605\n",
      "   4.912655   4.      ] (1000, 38) (1000, 1)\n"
     ]
    }
   ],
   "source": [
    "y_data = np.c_[telco[:,41]]\n",
    "x_rows = y_data.shape[0]\n",
    "x_data = np.c_[telco[:,0:35],telco[:,39:41]]\n",
    "x_data = np.c_[np.ones((x_rows,1)),x_data] # 矩阵拼接https://blog.csdn.net/ljxopencv/article/details/90548414 \n",
    "\n",
    "x_cols = x_data.shape[1]\n",
    "print(x_data[1],x_data.shape,y_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6c7d18d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.         1.         0.14084507 0.25423729 1.         0.12727273\n",
      " 0.07655214 1.         0.10638298 0.         0.         0.71428571\n",
      " 1.         0.         1.         1.         0.03533569 0.1199422\n",
      " 0.         0.1395881  0.31889236 0.00566373 0.03574206 0.\n",
      " 0.0166334  0.04840999 0.         1.         1.         0.\n",
      " 1.         1.         1.         1.         0.         0.33693324\n",
      " 0.51998255 1.        ] (1000, 38) (1000, 1)\n"
     ]
    }
   ],
   "source": [
    "# 区间变换,[0,100]化为[-1,1] 归一化 后面运行后发现cost变大, 想起来做个归一化会好一些 到时候可以写成函数重复利用率高一点\n",
    "for i in range(1,38):\n",
    "    x_data[:,i] = (x_data[:,i] -x_data[:,i].min())/(x_data[:,i].max()-x_data[:,i].min())\n",
    "print(x_data[1],x_data.shape,y_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a533a80",
   "metadata": {},
   "source": [
    "分出训练集和测试集, 那个valid集还不清楚是干什么的好像是训练过程中验证用的?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cf29a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mid = (int)(x_rows/2)\n",
    "x_train = x_data[0:mid,:]\n",
    "y_train = y_data[0:mid,:]\n",
    "x_test = x_data[mid:x_rows]\n",
    "y_test = y_data[mid:x_rows]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f686d7",
   "metadata": {},
   "source": [
    "sigmoid 函数和 偏差值计算函数 定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c524bf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "#z = X*theta.T \n",
    "#z = X*theta.T > 0 g(z) = hθ(x) >= 0.5  y = 1\n",
    "#z = X*theta.T > 0 g(z) = hθ(x) <  0.5  y = 0\n",
    "\n",
    "#简化\n",
    "#X*theta.T >= 0   y = 1\n",
    "#X*theta.T <  0   y = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38f6e4e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x12065bb5bc8>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 测试一下，保证Sigmoid函数正确性\n",
    "x1 = np.arange(-10, 10, 1)\n",
    "plt.plot(x1, sigmoid(x1), c='r')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "453358b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCostLogis(X,y,theta):\n",
    "    # 求偏差\n",
    "    m = len(y) \n",
    "#     inner = np.power(((X.dot(theta)) - y), 2) # (pred - y)^2\n",
    "#     print(X.shape,theta.shape,y.shape)\n",
    "#     print(y.shape)\n",
    "#     print(X.dot(theta).shape)\n",
    "#     print(np.log(sigmoid(X.dot(theta))).shape)\n",
    "#     print((1-y).shape)\n",
    "#     print(m)\n",
    "#     print(inner.shape)\n",
    "    inner = y.T.dot(np.log(sigmoid(X.dot(theta))))+ (1-y).T.dot(np.log(1-sigmoid(X.dot(theta))))\n",
    "    return  -1*np.sum(inner)/m;\n",
    "\n",
    "# # 2)Cost Fuc\n",
    "# def cost( X, y,theta):\n",
    "#     first = (-y) * np.log(sigmoid(X @ theta))\n",
    "#     second = (1 - y)*np.log(1 - sigmoid(X @ theta))\n",
    "#     return np.mean(first - second)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22fde85",
   "metadata": {},
   "source": [
    "小测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60879f44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38, 1) [1.         0.5        0.16901408 0.44067797 1.         0.16363636\n",
      " 0.0331525  0.75       0.10638298 0.         0.         0.14285714\n",
      " 0.         0.         1.         0.         0.02826855 0.\n",
      " 0.         0.06864989 0.         0.00503672 0.         0.\n",
      " 0.01463739 0.         0.         0.         0.         0.\n",
      " 0.         0.         1.         0.         0.         0.30014537\n",
      " 0.3756414  0.        ]\n"
     ]
    }
   ],
   "source": [
    "theta = np.zeros((x_cols,1))\n",
    "print(theta.shape,x_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fd3c063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n",
      "0.6931471805599446\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cost = computeCostLogis(x_data,y_data,theta)\n",
    "print(cost.shape)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4c4788",
   "metadata": {},
   "source": [
    "假设函数 \n",
    "梯度下降函数\n",
    "的定义\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "344ee041",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0107bbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypothesis(z):\n",
    "    return sigmoid(z);# 3)Gradient Fuc\n",
    "'''\n",
    "batch gradient descent\n",
    "转为向量化计算\n",
    "'''\n",
    "\n",
    "\n",
    "def gradient(X, y,theta):\n",
    "    # the gradient of the cost is a vector of the same length as θ where the jth element (for j = 0, 1, . . . , n)\n",
    "    return (X.T @ (hypothesis(X @ theta) - y)) / len(X)\n",
    "# @相当于.dot()\n",
    "\n",
    "def gradientDescent(X, y, theta, learning_rate, num_iters, tol,print_step):\n",
    "    #参数说明: X数据, y预测目标值, theta参数,learinig_rate 学习率  num_iters迭代次数 tol精度(0.01)\n",
    "    m = len(y)\n",
    "    J_history = np.zeros((num_iters, 1))\n",
    "    Grad_hist = np.zeros((num_iters, 1))\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        n = len(theta)\n",
    "        theta_temp = theta\n",
    "#         print((X.dot(theta)-y).shape)\n",
    "#         print(X[:,1].shape)\n",
    "#         print(X.shape,theta.shape,(X.dot(theta)-y).shape,y.shape,sigmoid(X.dot(theta)-y).shape)\n",
    "        for j in range(n):\n",
    "            theta_temp[j] = theta[j]+ learning_rate*(y-hypothesis(X.dot(theta))).T.dot(X[:,j])\n",
    "#             theta_temp[j] = theta[j] - learning_rate/m*(X.dot(theta)-y).T.dot(X[:,j]); \n",
    "        theta = theta_temp \n",
    "        cost = computeCostLogis(X, y, theta)\n",
    "        J_history[i] = cost\n",
    "        grad = np.sum(gradient(X, y,theta))\n",
    "        Grad_hist[i] = grad\n",
    "        if(i%print_step==0):\n",
    "            print('第%d次迭代, cost = %f grad =%f' %(i,cost,grad))\n",
    "#         if(cost < tol): #如果 cost 小于精确值 则退出\n",
    "        if(grad < tol): #如果 梯度 小于精确值 则退出\n",
    "            print('迭代训练结束,迭代次数:%d, 偏差值cost=%f grad =%f'%(i,cost,grad))\n",
    "            return (theta, i, J_history,Grad_hist)\n",
    "    \n",
    "    return (theta,i,J_history,Grad_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cddbe01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X= x_train\n",
    "y = y_train\n",
    "learning_rate = 0.001\n",
    "num_iters = 20000\n",
    "tol = 0.0001\n",
    "print_step = 50\n",
    "theta = np.zeros((x_cols,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64057bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0次迭代, cost = 0.590294 grad =1.398392\n",
      "第50次迭代, cost = 0.439701 grad =0.035193\n",
      "第100次迭代, cost = 0.425830 grad =0.024122\n",
      "第150次迭代, cost = 0.419714 grad =0.018517\n",
      "第200次迭代, cost = 0.416041 grad =0.015051\n",
      "第250次迭代, cost = 0.413512 grad =0.012661\n",
      "第300次迭代, cost = 0.411641 grad =0.010892\n",
      "第350次迭代, cost = 0.410187 grad =0.009515\n",
      "第400次迭代, cost = 0.409017 grad =0.008408\n",
      "第450次迭代, cost = 0.408049 grad =0.007496\n",
      "第500次迭代, cost = 0.407228 grad =0.006734\n",
      "第550次迭代, cost = 0.406519 grad =0.006087\n",
      "第600次迭代, cost = 0.405898 grad =0.005535\n",
      "第650次迭代, cost = 0.405348 grad =0.005059\n",
      "第700次迭代, cost = 0.404854 grad =0.004647\n",
      "第750次迭代, cost = 0.404408 grad =0.004288\n",
      "第800次迭代, cost = 0.404003 grad =0.003973\n",
      "第850次迭代, cost = 0.403632 grad =0.003697\n",
      "第900次迭代, cost = 0.403291 grad =0.003454\n",
      "第950次迭代, cost = 0.402975 grad =0.003238\n",
      "第1000次迭代, cost = 0.402684 grad =0.003046\n",
      "第1050次迭代, cost = 0.402412 grad =0.002875\n",
      "第1100次迭代, cost = 0.402159 grad =0.002723\n",
      "第1150次迭代, cost = 0.401922 grad =0.002586\n",
      "第1200次迭代, cost = 0.401700 grad =0.002462\n",
      "第1250次迭代, cost = 0.401491 grad =0.002351\n",
      "第1300次迭代, cost = 0.401295 grad =0.002251\n",
      "第1350次迭代, cost = 0.401110 grad =0.002160\n",
      "第1400次迭代, cost = 0.400935 grad =0.002078\n",
      "第1450次迭代, cost = 0.400769 grad =0.002002\n",
      "第1500次迭代, cost = 0.400612 grad =0.001934\n",
      "第1550次迭代, cost = 0.400463 grad =0.001871\n",
      "第1600次迭代, cost = 0.400321 grad =0.001813\n",
      "第1650次迭代, cost = 0.400187 grad =0.001761\n",
      "第1700次迭代, cost = 0.400058 grad =0.001712\n",
      "第1750次迭代, cost = 0.399935 grad =0.001667\n",
      "第1800次迭代, cost = 0.399818 grad =0.001626\n",
      "第1850次迭代, cost = 0.399706 grad =0.001588\n",
      "第1900次迭代, cost = 0.399598 grad =0.001552\n",
      "第1950次迭代, cost = 0.399496 grad =0.001519\n",
      "第2000次迭代, cost = 0.399397 grad =0.001489\n",
      "第2050次迭代, cost = 0.399302 grad =0.001460\n",
      "第2100次迭代, cost = 0.399211 grad =0.001434\n",
      "第2150次迭代, cost = 0.399123 grad =0.001409\n",
      "第2200次迭代, cost = 0.399039 grad =0.001386\n",
      "第2250次迭代, cost = 0.398958 grad =0.001364\n",
      "第2300次迭代, cost = 0.398879 grad =0.001344\n",
      "第2350次迭代, cost = 0.398804 grad =0.001325\n",
      "第2400次迭代, cost = 0.398731 grad =0.001307\n",
      "第2450次迭代, cost = 0.398660 grad =0.001290\n",
      "第2500次迭代, cost = 0.398592 grad =0.001274\n",
      "第2550次迭代, cost = 0.398526 grad =0.001259\n",
      "第2600次迭代, cost = 0.398462 grad =0.001244\n",
      "第2650次迭代, cost = 0.398400 grad =0.001231\n",
      "第2700次迭代, cost = 0.398340 grad =0.001218\n",
      "第2750次迭代, cost = 0.398282 grad =0.001206\n",
      "第2800次迭代, cost = 0.398225 grad =0.001194\n",
      "第2850次迭代, cost = 0.398171 grad =0.001183\n",
      "第2900次迭代, cost = 0.398117 grad =0.001172\n",
      "第2950次迭代, cost = 0.398065 grad =0.001162\n",
      "第3000次迭代, cost = 0.398015 grad =0.001152\n",
      "第3050次迭代, cost = 0.397966 grad =0.001143\n",
      "第3100次迭代, cost = 0.397918 grad =0.001134\n",
      "第3150次迭代, cost = 0.397871 grad =0.001125\n",
      "第3200次迭代, cost = 0.397826 grad =0.001116\n",
      "第3250次迭代, cost = 0.397781 grad =0.001108\n",
      "第3300次迭代, cost = 0.397738 grad =0.001101\n",
      "第3350次迭代, cost = 0.397696 grad =0.001093\n",
      "第3400次迭代, cost = 0.397654 grad =0.001086\n",
      "第3450次迭代, cost = 0.397614 grad =0.001079\n",
      "第3500次迭代, cost = 0.397575 grad =0.001072\n",
      "第3550次迭代, cost = 0.397536 grad =0.001065\n",
      "第3600次迭代, cost = 0.397498 grad =0.001058\n",
      "第3650次迭代, cost = 0.397461 grad =0.001052\n",
      "第3700次迭代, cost = 0.397425 grad =0.001046\n",
      "第3750次迭代, cost = 0.397390 grad =0.001040\n",
      "第3800次迭代, cost = 0.397355 grad =0.001034\n",
      "第3850次迭代, cost = 0.397321 grad =0.001028\n",
      "第3900次迭代, cost = 0.397288 grad =0.001022\n",
      "第3950次迭代, cost = 0.397255 grad =0.001016\n",
      "第4000次迭代, cost = 0.397223 grad =0.001011\n",
      "第4050次迭代, cost = 0.397191 grad =0.001005\n",
      "第4100次迭代, cost = 0.397160 grad =0.001000\n",
      "第4150次迭代, cost = 0.397130 grad =0.000995\n",
      "第4200次迭代, cost = 0.397100 grad =0.000990\n",
      "第4250次迭代, cost = 0.397070 grad =0.000985\n",
      "第4300次迭代, cost = 0.397041 grad =0.000980\n",
      "第4350次迭代, cost = 0.397013 grad =0.000975\n",
      "第4400次迭代, cost = 0.396985 grad =0.000970\n",
      "第4450次迭代, cost = 0.396958 grad =0.000965\n",
      "第4500次迭代, cost = 0.396930 grad =0.000960\n",
      "第4550次迭代, cost = 0.396904 grad =0.000956\n",
      "第4600次迭代, cost = 0.396877 grad =0.000951\n",
      "第4650次迭代, cost = 0.396852 grad =0.000946\n",
      "第4700次迭代, cost = 0.396826 grad =0.000942\n",
      "第4750次迭代, cost = 0.396801 grad =0.000937\n",
      "第4800次迭代, cost = 0.396776 grad =0.000933\n",
      "第4850次迭代, cost = 0.396752 grad =0.000928\n",
      "第4900次迭代, cost = 0.396728 grad =0.000924\n",
      "第4950次迭代, cost = 0.396704 grad =0.000920\n",
      "第5000次迭代, cost = 0.396680 grad =0.000916\n",
      "第5050次迭代, cost = 0.396657 grad =0.000911\n",
      "第5100次迭代, cost = 0.396634 grad =0.000907\n",
      "第5150次迭代, cost = 0.396612 grad =0.000903\n",
      "第5200次迭代, cost = 0.396590 grad =0.000899\n",
      "第5250次迭代, cost = 0.396568 grad =0.000895\n",
      "第5300次迭代, cost = 0.396546 grad =0.000891\n",
      "第5350次迭代, cost = 0.396524 grad =0.000887\n",
      "第5400次迭代, cost = 0.396503 grad =0.000883\n",
      "第5450次迭代, cost = 0.396482 grad =0.000879\n",
      "第5500次迭代, cost = 0.396461 grad =0.000875\n",
      "第5550次迭代, cost = 0.396441 grad =0.000871\n",
      "第5600次迭代, cost = 0.396421 grad =0.000867\n",
      "第5650次迭代, cost = 0.396401 grad =0.000863\n",
      "第5700次迭代, cost = 0.396381 grad =0.000860\n",
      "第5750次迭代, cost = 0.396361 grad =0.000856\n",
      "第5800次迭代, cost = 0.396342 grad =0.000852\n",
      "第5850次迭代, cost = 0.396323 grad =0.000849\n",
      "第5900次迭代, cost = 0.396304 grad =0.000845\n",
      "第5950次迭代, cost = 0.396285 grad =0.000841\n",
      "第6000次迭代, cost = 0.396267 grad =0.000838\n",
      "第6050次迭代, cost = 0.396248 grad =0.000834\n",
      "第6100次迭代, cost = 0.396230 grad =0.000831\n",
      "第6150次迭代, cost = 0.396212 grad =0.000827\n",
      "第6200次迭代, cost = 0.396194 grad =0.000824\n",
      "第6250次迭代, cost = 0.396176 grad =0.000820\n",
      "第6300次迭代, cost = 0.396159 grad =0.000817\n",
      "第6350次迭代, cost = 0.396142 grad =0.000813\n",
      "第6400次迭代, cost = 0.396124 grad =0.000810\n",
      "第6450次迭代, cost = 0.396107 grad =0.000807\n",
      "第6500次迭代, cost = 0.396091 grad =0.000803\n",
      "第6550次迭代, cost = 0.396074 grad =0.000800\n",
      "第6600次迭代, cost = 0.396057 grad =0.000797\n",
      "第6650次迭代, cost = 0.396041 grad =0.000793\n",
      "第6700次迭代, cost = 0.396025 grad =0.000790\n",
      "第6750次迭代, cost = 0.396009 grad =0.000787\n",
      "第6800次迭代, cost = 0.395993 grad =0.000784\n",
      "第6850次迭代, cost = 0.395977 grad =0.000781\n",
      "第6900次迭代, cost = 0.395961 grad =0.000778\n",
      "第6950次迭代, cost = 0.395946 grad =0.000774\n",
      "第7000次迭代, cost = 0.395930 grad =0.000771\n",
      "第7050次迭代, cost = 0.395915 grad =0.000768\n",
      "第7100次迭代, cost = 0.395900 grad =0.000765\n",
      "第7150次迭代, cost = 0.395885 grad =0.000762\n",
      "第7200次迭代, cost = 0.395870 grad =0.000759\n",
      "第7250次迭代, cost = 0.395855 grad =0.000756\n",
      "第7300次迭代, cost = 0.395840 grad =0.000753\n",
      "第7350次迭代, cost = 0.395826 grad =0.000751\n",
      "第7400次迭代, cost = 0.395811 grad =0.000748\n",
      "第7450次迭代, cost = 0.395797 grad =0.000745\n",
      "第7500次迭代, cost = 0.395783 grad =0.000742\n",
      "第7550次迭代, cost = 0.395768 grad =0.000739\n",
      "第7600次迭代, cost = 0.395754 grad =0.000736\n",
      "第7650次迭代, cost = 0.395741 grad =0.000734\n",
      "第7700次迭代, cost = 0.395727 grad =0.000731\n",
      "第7750次迭代, cost = 0.395713 grad =0.000728\n",
      "第7800次迭代, cost = 0.395699 grad =0.000725\n",
      "第7850次迭代, cost = 0.395686 grad =0.000723\n",
      "第7900次迭代, cost = 0.395673 grad =0.000720\n",
      "第7950次迭代, cost = 0.395659 grad =0.000717\n",
      "第8000次迭代, cost = 0.395646 grad =0.000715\n",
      "第8050次迭代, cost = 0.395633 grad =0.000712\n",
      "第8100次迭代, cost = 0.395620 grad =0.000710\n",
      "第8150次迭代, cost = 0.395607 grad =0.000707\n",
      "第8200次迭代, cost = 0.395594 grad =0.000704\n",
      "第8250次迭代, cost = 0.395581 grad =0.000702\n",
      "第8300次迭代, cost = 0.395569 grad =0.000699\n",
      "第8350次迭代, cost = 0.395556 grad =0.000697\n",
      "第8400次迭代, cost = 0.395544 grad =0.000694\n",
      "第8450次迭代, cost = 0.395531 grad =0.000692\n",
      "第8500次迭代, cost = 0.395519 grad =0.000690\n",
      "第8550次迭代, cost = 0.395507 grad =0.000687\n",
      "第8600次迭代, cost = 0.395495 grad =0.000685\n",
      "第8650次迭代, cost = 0.395482 grad =0.000682\n",
      "第8700次迭代, cost = 0.395470 grad =0.000680\n",
      "第8750次迭代, cost = 0.395459 grad =0.000678\n",
      "第8800次迭代, cost = 0.395447 grad =0.000675\n",
      "第8850次迭代, cost = 0.395435 grad =0.000673\n",
      "第8900次迭代, cost = 0.395423 grad =0.000671\n",
      "第8950次迭代, cost = 0.395412 grad =0.000668\n",
      "第9000次迭代, cost = 0.395400 grad =0.000666\n",
      "第9050次迭代, cost = 0.395389 grad =0.000664\n",
      "第9100次迭代, cost = 0.395377 grad =0.000662\n",
      "第9150次迭代, cost = 0.395366 grad =0.000660\n",
      "第9200次迭代, cost = 0.395355 grad =0.000657\n",
      "第9250次迭代, cost = 0.395344 grad =0.000655\n",
      "第9300次迭代, cost = 0.395332 grad =0.000653\n",
      "第9350次迭代, cost = 0.395321 grad =0.000651\n",
      "第9400次迭代, cost = 0.395310 grad =0.000649\n",
      "第9450次迭代, cost = 0.395300 grad =0.000647\n",
      "第9500次迭代, cost = 0.395289 grad =0.000644\n",
      "第9550次迭代, cost = 0.395278 grad =0.000642\n",
      "第9600次迭代, cost = 0.395267 grad =0.000640\n",
      "第9650次迭代, cost = 0.395257 grad =0.000638\n",
      "第9700次迭代, cost = 0.395246 grad =0.000636\n",
      "第9750次迭代, cost = 0.395235 grad =0.000634\n",
      "第9800次迭代, cost = 0.395225 grad =0.000632\n",
      "第9850次迭代, cost = 0.395215 grad =0.000630\n",
      "第9900次迭代, cost = 0.395204 grad =0.000628\n",
      "第9950次迭代, cost = 0.395194 grad =0.000626\n",
      "第10000次迭代, cost = 0.395184 grad =0.000624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第10050次迭代, cost = 0.395174 grad =0.000622\n",
      "第10100次迭代, cost = 0.395164 grad =0.000620\n",
      "第10150次迭代, cost = 0.395154 grad =0.000618\n",
      "第10200次迭代, cost = 0.395144 grad =0.000617\n",
      "第10250次迭代, cost = 0.395134 grad =0.000615\n",
      "第10300次迭代, cost = 0.395124 grad =0.000613\n",
      "第10350次迭代, cost = 0.395114 grad =0.000611\n",
      "第10400次迭代, cost = 0.395104 grad =0.000609\n",
      "第10450次迭代, cost = 0.395094 grad =0.000607\n",
      "第10500次迭代, cost = 0.395085 grad =0.000605\n",
      "第10550次迭代, cost = 0.395075 grad =0.000604\n",
      "第10600次迭代, cost = 0.395066 grad =0.000602\n",
      "第10650次迭代, cost = 0.395056 grad =0.000600\n",
      "第10700次迭代, cost = 0.395047 grad =0.000598\n",
      "第10750次迭代, cost = 0.395037 grad =0.000596\n",
      "第10800次迭代, cost = 0.395028 grad =0.000595\n",
      "第10850次迭代, cost = 0.395019 grad =0.000593\n",
      "第10900次迭代, cost = 0.395009 grad =0.000591\n",
      "第10950次迭代, cost = 0.395000 grad =0.000589\n",
      "第11000次迭代, cost = 0.394991 grad =0.000588\n",
      "第11050次迭代, cost = 0.394982 grad =0.000586\n",
      "第11100次迭代, cost = 0.394973 grad =0.000584\n",
      "第11150次迭代, cost = 0.394964 grad =0.000583\n",
      "第11200次迭代, cost = 0.394955 grad =0.000581\n",
      "第11250次迭代, cost = 0.394946 grad =0.000579\n",
      "第11300次迭代, cost = 0.394937 grad =0.000578\n",
      "第11350次迭代, cost = 0.394928 grad =0.000576\n",
      "第11400次迭代, cost = 0.394920 grad =0.000574\n",
      "第11450次迭代, cost = 0.394911 grad =0.000573\n",
      "第11500次迭代, cost = 0.394902 grad =0.000571\n",
      "第11550次迭代, cost = 0.394893 grad =0.000570\n",
      "第11600次迭代, cost = 0.394885 grad =0.000568\n",
      "第11650次迭代, cost = 0.394876 grad =0.000566\n",
      "第11700次迭代, cost = 0.394868 grad =0.000565\n",
      "第11750次迭代, cost = 0.394859 grad =0.000563\n",
      "第11800次迭代, cost = 0.394851 grad =0.000562\n",
      "第11850次迭代, cost = 0.394842 grad =0.000560\n",
      "第11900次迭代, cost = 0.394834 grad =0.000559\n",
      "第11950次迭代, cost = 0.394826 grad =0.000557\n",
      "第12000次迭代, cost = 0.394818 grad =0.000556\n",
      "第12050次迭代, cost = 0.394809 grad =0.000554\n",
      "第12100次迭代, cost = 0.394801 grad =0.000553\n",
      "第12150次迭代, cost = 0.394793 grad =0.000551\n",
      "第12200次迭代, cost = 0.394785 grad =0.000550\n",
      "第12250次迭代, cost = 0.394777 grad =0.000548\n",
      "第12300次迭代, cost = 0.394769 grad =0.000547\n",
      "第12350次迭代, cost = 0.394761 grad =0.000545\n",
      "第12400次迭代, cost = 0.394753 grad =0.000544\n",
      "第12450次迭代, cost = 0.394745 grad =0.000542\n",
      "第12500次迭代, cost = 0.394737 grad =0.000541\n",
      "第12550次迭代, cost = 0.394729 grad =0.000539\n",
      "第12600次迭代, cost = 0.394721 grad =0.000538\n",
      "第12650次迭代, cost = 0.394714 grad =0.000537\n",
      "第12700次迭代, cost = 0.394706 grad =0.000535\n",
      "第12750次迭代, cost = 0.394698 grad =0.000534\n",
      "第12800次迭代, cost = 0.394690 grad =0.000532\n",
      "第12850次迭代, cost = 0.394683 grad =0.000531\n",
      "第12900次迭代, cost = 0.394675 grad =0.000530\n",
      "第12950次迭代, cost = 0.394668 grad =0.000528\n",
      "第13000次迭代, cost = 0.394660 grad =0.000527\n",
      "第13050次迭代, cost = 0.394653 grad =0.000525\n",
      "第13100次迭代, cost = 0.394645 grad =0.000524\n",
      "第13150次迭代, cost = 0.394638 grad =0.000523\n",
      "第13200次迭代, cost = 0.394630 grad =0.000521\n",
      "第13250次迭代, cost = 0.394623 grad =0.000520\n",
      "第13300次迭代, cost = 0.394616 grad =0.000519\n",
      "第13350次迭代, cost = 0.394608 grad =0.000517\n",
      "第13400次迭代, cost = 0.394601 grad =0.000516\n",
      "第13450次迭代, cost = 0.394594 grad =0.000515\n",
      "第13500次迭代, cost = 0.394587 grad =0.000514\n",
      "第13550次迭代, cost = 0.394579 grad =0.000512\n",
      "第13600次迭代, cost = 0.394572 grad =0.000511\n",
      "第13650次迭代, cost = 0.394565 grad =0.000510\n",
      "第13700次迭代, cost = 0.394558 grad =0.000508\n",
      "第13750次迭代, cost = 0.394551 grad =0.000507\n",
      "第13800次迭代, cost = 0.394544 grad =0.000506\n",
      "第13850次迭代, cost = 0.394537 grad =0.000505\n",
      "第13900次迭代, cost = 0.394530 grad =0.000503\n",
      "第13950次迭代, cost = 0.394523 grad =0.000502\n",
      "第14000次迭代, cost = 0.394516 grad =0.000501\n",
      "第14050次迭代, cost = 0.394509 grad =0.000500\n",
      "第14100次迭代, cost = 0.394503 grad =0.000498\n",
      "第14150次迭代, cost = 0.394496 grad =0.000497\n",
      "第14200次迭代, cost = 0.394489 grad =0.000496\n",
      "第14250次迭代, cost = 0.394482 grad =0.000495\n",
      "第14300次迭代, cost = 0.394476 grad =0.000494\n",
      "第14350次迭代, cost = 0.394469 grad =0.000492\n",
      "第14400次迭代, cost = 0.394462 grad =0.000491\n",
      "第14450次迭代, cost = 0.394456 grad =0.000490\n",
      "第14500次迭代, cost = 0.394449 grad =0.000489\n",
      "第14550次迭代, cost = 0.394442 grad =0.000488\n",
      "第14600次迭代, cost = 0.394436 grad =0.000487\n",
      "第14650次迭代, cost = 0.394429 grad =0.000485\n",
      "第14700次迭代, cost = 0.394423 grad =0.000484\n",
      "第14750次迭代, cost = 0.394416 grad =0.000483\n",
      "第14800次迭代, cost = 0.394410 grad =0.000482\n",
      "第14850次迭代, cost = 0.394403 grad =0.000481\n",
      "第14900次迭代, cost = 0.394397 grad =0.000480\n",
      "第14950次迭代, cost = 0.394391 grad =0.000479\n",
      "第15000次迭代, cost = 0.394384 grad =0.000477\n",
      "第15050次迭代, cost = 0.394378 grad =0.000476\n",
      "第15100次迭代, cost = 0.394372 grad =0.000475\n",
      "第15150次迭代, cost = 0.394365 grad =0.000474\n",
      "第15200次迭代, cost = 0.394359 grad =0.000473\n",
      "第15250次迭代, cost = 0.394353 grad =0.000472\n",
      "第15300次迭代, cost = 0.394347 grad =0.000471\n",
      "第15350次迭代, cost = 0.394340 grad =0.000470\n",
      "第15400次迭代, cost = 0.394334 grad =0.000469\n",
      "第15450次迭代, cost = 0.394328 grad =0.000468\n",
      "第15500次迭代, cost = 0.394322 grad =0.000466\n",
      "第15550次迭代, cost = 0.394316 grad =0.000465\n",
      "第15600次迭代, cost = 0.394310 grad =0.000464\n",
      "第15650次迭代, cost = 0.394304 grad =0.000463\n",
      "第15700次迭代, cost = 0.394298 grad =0.000462\n",
      "第15750次迭代, cost = 0.394292 grad =0.000461\n",
      "第15800次迭代, cost = 0.394286 grad =0.000460\n",
      "第15850次迭代, cost = 0.394280 grad =0.000459\n",
      "第15900次迭代, cost = 0.394274 grad =0.000458\n",
      "第15950次迭代, cost = 0.394268 grad =0.000457\n",
      "第16000次迭代, cost = 0.394262 grad =0.000456\n",
      "第16050次迭代, cost = 0.394256 grad =0.000455\n",
      "第16100次迭代, cost = 0.394250 grad =0.000454\n",
      "第16150次迭代, cost = 0.394245 grad =0.000453\n",
      "第16200次迭代, cost = 0.394239 grad =0.000452\n",
      "第16250次迭代, cost = 0.394233 grad =0.000451\n",
      "第16300次迭代, cost = 0.394227 grad =0.000450\n",
      "第16350次迭代, cost = 0.394222 grad =0.000449\n",
      "第16400次迭代, cost = 0.394216 grad =0.000448\n",
      "第16450次迭代, cost = 0.394210 grad =0.000447\n",
      "第16500次迭代, cost = 0.394205 grad =0.000446\n",
      "第16550次迭代, cost = 0.394199 grad =0.000445\n",
      "第16600次迭代, cost = 0.394193 grad =0.000444\n",
      "第16650次迭代, cost = 0.394188 grad =0.000443\n",
      "第16700次迭代, cost = 0.394182 grad =0.000442\n",
      "第16750次迭代, cost = 0.394176 grad =0.000441\n",
      "第16800次迭代, cost = 0.394171 grad =0.000440\n",
      "第16850次迭代, cost = 0.394165 grad =0.000439\n",
      "第16900次迭代, cost = 0.394160 grad =0.000438\n",
      "第16950次迭代, cost = 0.394154 grad =0.000437\n",
      "第17000次迭代, cost = 0.394149 grad =0.000436\n",
      "第17050次迭代, cost = 0.394144 grad =0.000435\n",
      "第17100次迭代, cost = 0.394138 grad =0.000434\n",
      "第17150次迭代, cost = 0.394133 grad =0.000433\n",
      "第17200次迭代, cost = 0.394127 grad =0.000432\n",
      "第17250次迭代, cost = 0.394122 grad =0.000431\n",
      "第17300次迭代, cost = 0.394117 grad =0.000430\n",
      "第17350次迭代, cost = 0.394111 grad =0.000429\n",
      "第17400次迭代, cost = 0.394106 grad =0.000429\n",
      "第17450次迭代, cost = 0.394101 grad =0.000428\n",
      "第17500次迭代, cost = 0.394095 grad =0.000427\n",
      "第17550次迭代, cost = 0.394090 grad =0.000426\n",
      "第17600次迭代, cost = 0.394085 grad =0.000425\n",
      "第17650次迭代, cost = 0.394080 grad =0.000424\n",
      "第17700次迭代, cost = 0.394074 grad =0.000423\n",
      "第17750次迭代, cost = 0.394069 grad =0.000422\n",
      "第17800次迭代, cost = 0.394064 grad =0.000421\n",
      "第17850次迭代, cost = 0.394059 grad =0.000420\n",
      "第17900次迭代, cost = 0.394054 grad =0.000419\n",
      "第17950次迭代, cost = 0.394049 grad =0.000419\n",
      "第18000次迭代, cost = 0.394044 grad =0.000418\n",
      "第18050次迭代, cost = 0.394038 grad =0.000417\n",
      "第18100次迭代, cost = 0.394033 grad =0.000416\n",
      "第18150次迭代, cost = 0.394028 grad =0.000415\n",
      "第18200次迭代, cost = 0.394023 grad =0.000414\n",
      "第18250次迭代, cost = 0.394018 grad =0.000413\n",
      "第18300次迭代, cost = 0.394013 grad =0.000412\n",
      "第18350次迭代, cost = 0.394008 grad =0.000412\n",
      "第18400次迭代, cost = 0.394003 grad =0.000411\n",
      "第18450次迭代, cost = 0.393998 grad =0.000410\n",
      "第18500次迭代, cost = 0.393993 grad =0.000409\n",
      "第18550次迭代, cost = 0.393989 grad =0.000408\n",
      "第18600次迭代, cost = 0.393984 grad =0.000407\n",
      "第18650次迭代, cost = 0.393979 grad =0.000406\n",
      "第18700次迭代, cost = 0.393974 grad =0.000406\n",
      "第18750次迭代, cost = 0.393969 grad =0.000405\n",
      "第18800次迭代, cost = 0.393964 grad =0.000404\n",
      "第18850次迭代, cost = 0.393959 grad =0.000403\n",
      "第18900次迭代, cost = 0.393955 grad =0.000402\n",
      "第18950次迭代, cost = 0.393950 grad =0.000401\n",
      "第19000次迭代, cost = 0.393945 grad =0.000401\n",
      "第19050次迭代, cost = 0.393940 grad =0.000400\n",
      "第19100次迭代, cost = 0.393935 grad =0.000399\n",
      "第19150次迭代, cost = 0.393931 grad =0.000398\n",
      "第19200次迭代, cost = 0.393926 grad =0.000397\n",
      "第19250次迭代, cost = 0.393921 grad =0.000397\n",
      "第19300次迭代, cost = 0.393917 grad =0.000396\n",
      "第19350次迭代, cost = 0.393912 grad =0.000395\n",
      "第19400次迭代, cost = 0.393907 grad =0.000394\n",
      "第19450次迭代, cost = 0.393903 grad =0.000393\n",
      "第19500次迭代, cost = 0.393898 grad =0.000393\n",
      "第19550次迭代, cost = 0.393893 grad =0.000392\n",
      "第19600次迭代, cost = 0.393889 grad =0.000391\n",
      "第19650次迭代, cost = 0.393884 grad =0.000390\n",
      "第19700次迭代, cost = 0.393880 grad =0.000389\n",
      "第19750次迭代, cost = 0.393875 grad =0.000389\n",
      "第19800次迭代, cost = 0.393871 grad =0.000388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第19850次迭代, cost = 0.393866 grad =0.000387\n",
      "第19900次迭代, cost = 0.393862 grad =0.000386\n",
      "第19950次迭代, cost = 0.393857 grad =0.000385\n"
     ]
    }
   ],
   "source": [
    "(theta,iters,J_hist,Grad_hist) = gradientDescent(X, y, theta, learning_rate, num_iters, tol,print_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84047435",
   "metadata": {},
   "source": [
    "测试后发现 第一训练 cost变大, 先加个归一化再说"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0a48ee0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x12065ca57c8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(J_hist.size)\n",
    "\n",
    "x_axis = np.linspace(0,iters,iters)\n",
    "plt.plot(x_axis,J_hist[0:iters],'+')\n",
    "plt.plot(x_axis,Grad_hist[0:iters],'.')\n",
    "plt.legend(['cost','grad'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add6ff24",
   "metadata": {},
   "source": [
    "准确率 78左右"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1aa60af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "73.6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def predict( X,theta):\n",
    "    probability = sigmoid(X @ theta)\n",
    "    return [1 if x >= 0.5 else 0 for x in probability]  # return a list\n",
    "\n",
    "\n",
    "# learning_parameters = np.array([-25.1613186, 0.20623159, 0.20147149])\n",
    "X = x_test\n",
    "predictions = predict( X,theta)\n",
    "print(predictions)\n",
    "correct = []\n",
    "for i in range(len(predictions)):\n",
    "    if predictions[i] == y_test[i]:\n",
    "        correct.append(1)\n",
    "    else:\n",
    "        correct.append(0)\n",
    "        pass\n",
    "    pass\n",
    "# correct = [1 if a == b else 0 for (a, b) in zip(predictions, y)] 这样也可以返回一个list\n",
    "accuracy = sum(correct)/len(X)\n",
    "print(accuracy*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3622f9",
   "metadata": {},
   "source": [
    "## 牛顿法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9b10fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 38) (38, 1)\n",
      "(500, 1)\n",
      "(500, 500)\n",
      "[[0.25 0.   0.   ... 0.   0.   0.  ]\n",
      " [0.   0.25 0.   ... 0.   0.   0.  ]\n",
      " [0.   0.   0.25 ... 0.   0.   0.  ]\n",
      " ...\n",
      " [0.   0.   0.   ... 0.25 0.   0.  ]\n",
      " [0.   0.   0.   ... 0.   0.25 0.  ]\n",
      " [0.   0.   0.   ... 0.   0.   0.25]]\n"
     ]
    }
   ],
   "source": [
    "# 学习痕迹  学习python 搞成单位矩阵\n",
    "X= x_train\n",
    "y = y_train\n",
    "learning_rate = 0.001\n",
    "num_iters = 20000\n",
    "tol = 0.0001\n",
    "print_step = 50\n",
    "theta = np.zeros((x_cols,1))\n",
    "print(X.shape,theta.shape)\n",
    "d = hypothesis(X.dot(theta))*(1-hypothesis(X.dot(theta)))\n",
    "print(d.shape)\n",
    "D = np.diag(d.flatten())\n",
    "print(D.shape)\n",
    "print(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "419032e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def newtonsMethod(X, y, theta, learning_rate, num_iters, tol,print_step):\n",
    "    #参数说明: X数据, y预测目标值, theta参数,learinig_rate 学习率  num_iters迭代次数 tol精度(0.01)\n",
    "    m = len(y)\n",
    "    J_history = np.zeros((num_iters, 1))\n",
    "    Grad_hist = np.zeros((num_iters, 1))\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        n = len(theta)\n",
    "        d = hypothesis(X.dot(theta))*(1-hypothesis(X.dot(theta)))\n",
    "        D = np.diag(d.flatten())\n",
    "        H = X.T.dot(D).dot(X)\n",
    "        grad = gradient(X,y,theta)\n",
    "        \n",
    "        theta = theta - np.linalg.pinv(H).dot(grad)\n",
    "        cost = computeCostLogis(X, y, theta)\n",
    "        J_history[i] = cost\n",
    "        grad = np.sum(gradient(X, y,theta))\n",
    "        Grad_hist[i] = grad\n",
    "        if(i%print_step==0):\n",
    "            print('第%d次迭代, cost = %f grad =%f' %(i,cost,grad))\n",
    "#         if(cost < tol): #如果 cost 小于精确值 则退出\n",
    "        if(grad < tol): #如果 梯度 小于精确值 则退出\n",
    "            print('迭代训练结束,迭代次数:%d, 偏差值cost=%f grad =%f'%(i,cost,grad))\n",
    "            return (theta, i, J_history,Grad_hist)\n",
    "    \n",
    "    return (theta,i,J_history,Grad_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "590963d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X= x_train\n",
    "y = y_train\n",
    "learning_rate = 0.001\n",
    "num_iters = 20000\n",
    "tol = 0.0001\n",
    "print_step = 50\n",
    "theta = np.zeros((x_cols,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f61a316b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0次迭代, cost = 0.692217 grad =3.054537\n",
      "第50次迭代, cost = 0.650031 grad =2.763605\n",
      "第100次迭代, cost = 0.614932 grad =2.500416\n",
      "第150次迭代, cost = 0.585352 grad =2.262314\n",
      "第200次迭代, cost = 0.560174 grad =2.046900\n",
      "第250次迭代, cost = 0.538573 grad =1.852010\n",
      "第300次迭代, cost = 0.519927 grad =1.675683\n",
      "第350次迭代, cost = 0.503751 grad =1.516150\n",
      "第400次迭代, cost = 0.489666 grad =1.371809\n",
      "第450次迭代, cost = 0.477363 grad =1.241212\n",
      "第500次迭代, cost = 0.466594 grad =1.123050\n",
      "第550次迭代, cost = 0.457153 grad =1.016137\n",
      "第600次迭代, cost = 0.448866 grad =0.919404\n",
      "第650次迭代, cost = 0.441589 grad =0.831879\n",
      "第700次迭代, cost = 0.435197 grad =0.752686\n",
      "第750次迭代, cost = 0.429583 grad =0.681032\n",
      "第800次迭代, cost = 0.424656 grad =0.616199\n",
      "第850次迭代, cost = 0.420333 grad =0.557538\n",
      "第900次迭代, cost = 0.416544 grad =0.504460\n",
      "第950次迭代, cost = 0.413226 grad =0.456435\n",
      "第1000次迭代, cost = 0.410325 grad =0.412982\n",
      "第1050次迭代, cost = 0.407790 grad =0.373665\n",
      "第1100次迭代, cost = 0.405578 grad =0.338090\n",
      "第1150次迭代, cost = 0.403651 grad =0.305902\n",
      "第1200次迭代, cost = 0.401974 grad =0.276778\n",
      "第1250次迭代, cost = 0.400516 grad =0.250427\n",
      "第1300次迭代, cost = 0.399251 grad =0.226584\n",
      "第1350次迭代, cost = 0.398153 grad =0.205011\n",
      "第1400次迭代, cost = 0.397203 grad =0.185491\n",
      "第1450次迭代, cost = 0.396381 grad =0.167830\n",
      "第1500次迭代, cost = 0.395670 grad =0.151850\n",
      "第1550次迭代, cost = 0.395056 grad =0.137391\n",
      "第1600次迭代, cost = 0.394527 grad =0.124309\n",
      "第1650次迭代, cost = 0.394071 grad =0.112473\n",
      "第1700次迭代, cost = 0.393679 grad =0.101763\n",
      "第1750次迭代, cost = 0.393342 grad =0.092073\n",
      "第1800次迭代, cost = 0.393052 grad =0.083305\n",
      "第1850次迭代, cost = 0.392803 grad =0.075373\n",
      "第1900次迭代, cost = 0.392589 grad =0.068195\n",
      "第1950次迭代, cost = 0.392406 grad =0.061701\n",
      "第2000次迭代, cost = 0.392249 grad =0.055825\n",
      "第2050次迭代, cost = 0.392114 grad =0.050509\n",
      "第2100次迭代, cost = 0.391999 grad =0.045699\n",
      "第2150次迭代, cost = 0.391900 grad =0.041347\n",
      "第2200次迭代, cost = 0.391815 grad =0.037410\n",
      "第2250次迭代, cost = 0.391743 grad =0.033847\n",
      "第2300次迭代, cost = 0.391680 grad =0.030623\n",
      "第2350次迭代, cost = 0.391626 grad =0.027707\n",
      "第2400次迭代, cost = 0.391580 grad =0.025068\n",
      "第2450次迭代, cost = 0.391541 grad =0.022681\n",
      "第2500次迭代, cost = 0.391506 grad =0.020521\n",
      "第2550次迭代, cost = 0.391477 grad =0.018566\n",
      "第2600次迭代, cost = 0.391451 grad =0.016798\n",
      "第2650次迭代, cost = 0.391429 grad =0.015198\n",
      "第2700次迭代, cost = 0.391409 grad =0.013751\n",
      "第2750次迭代, cost = 0.391393 grad =0.012441\n",
      "第2800次迭代, cost = 0.391378 grad =0.011256\n",
      "第2850次迭代, cost = 0.391365 grad =0.010184\n",
      "第2900次迭代, cost = 0.391354 grad =0.009214\n",
      "第2950次迭代, cost = 0.391344 grad =0.008337\n",
      "第3000次迭代, cost = 0.391336 grad =0.007543\n",
      "第3050次迭代, cost = 0.391328 grad =0.006824\n",
      "第3100次迭代, cost = 0.391321 grad =0.006174\n",
      "第3150次迭代, cost = 0.391316 grad =0.005586\n",
      "第3200次迭代, cost = 0.391310 grad =0.005054\n",
      "第3250次迭代, cost = 0.391306 grad =0.004573\n",
      "第3300次迭代, cost = 0.391302 grad =0.004137\n",
      "第3350次迭代, cost = 0.391298 grad =0.003743\n",
      "第3400次迭代, cost = 0.391295 grad =0.003387\n",
      "第3450次迭代, cost = 0.391292 grad =0.003064\n",
      "第3500次迭代, cost = 0.391289 grad =0.002772\n",
      "第3550次迭代, cost = 0.391287 grad =0.002508\n",
      "第3600次迭代, cost = 0.391285 grad =0.002269\n",
      "第3650次迭代, cost = 0.391283 grad =0.002053\n",
      "第3700次迭代, cost = 0.391282 grad =0.001858\n",
      "第3750次迭代, cost = 0.391280 grad =0.001681\n",
      "第3800次迭代, cost = 0.391279 grad =0.001521\n",
      "第3850次迭代, cost = 0.391278 grad =0.001376\n",
      "第3900次迭代, cost = 0.391276 grad =0.001245\n",
      "第3950次迭代, cost = 0.391276 grad =0.001126\n",
      "第4000次迭代, cost = 0.391275 grad =0.001019\n",
      "第4050次迭代, cost = 0.391274 grad =0.000922\n",
      "第4100次迭代, cost = 0.391273 grad =0.000834\n",
      "第4150次迭代, cost = 0.391273 grad =0.000755\n",
      "第4200次迭代, cost = 0.391272 grad =0.000683\n",
      "第4250次迭代, cost = 0.391272 grad =0.000618\n",
      "第4300次迭代, cost = 0.391271 grad =0.000559\n",
      "第4350次迭代, cost = 0.391271 grad =0.000506\n",
      "第4400次迭代, cost = 0.391270 grad =0.000458\n",
      "第4450次迭代, cost = 0.391270 grad =0.000414\n",
      "第4500次迭代, cost = 0.391270 grad =0.000375\n",
      "第4550次迭代, cost = 0.391269 grad =0.000339\n",
      "第4600次迭代, cost = 0.391269 grad =0.000307\n",
      "第4650次迭代, cost = 0.391269 grad =0.000277\n",
      "第4700次迭代, cost = 0.391269 grad =0.000251\n",
      "第4750次迭代, cost = 0.391268 grad =0.000227\n",
      "第4800次迭代, cost = 0.391268 grad =0.000205\n",
      "第4850次迭代, cost = 0.391268 grad =0.000186\n",
      "第4900次迭代, cost = 0.391268 grad =0.000168\n",
      "第4950次迭代, cost = 0.391268 grad =0.000152\n",
      "第5000次迭代, cost = 0.391268 grad =0.000138\n",
      "第5050次迭代, cost = 0.391268 grad =0.000125\n",
      "第5100次迭代, cost = 0.391268 grad =0.000113\n",
      "第5150次迭代, cost = 0.391268 grad =0.000102\n",
      "迭代训练结束,迭代次数:5160, 偏差值cost=0.391268 grad =0.000100\n"
     ]
    }
   ],
   "source": [
    "(theta,iters,J_history,Grad_hist) = newtonsMethod(X, y, theta, learning_rate, num_iters, tol,print_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c194e9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x12067ff3a08>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(J_history.size)\n",
    "\n",
    "x_axis = np.linspace(0,iters,iters)\n",
    "plt.plot(x_axis,J_history[0:iters],'+')\n",
    "plt.plot(x_axis,Grad_hist[0:iters],'.')\n",
    "plt.legend(['cost','grad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6e67d5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "72.6\n"
     ]
    }
   ],
   "source": [
    "def predict( X,theta):\n",
    "    probability = sigmoid(X.dot(theta))\n",
    "    return [1 if x >= 0.5 else 0 for x in probability]  # return a list\n",
    "\n",
    "# learning_parameters = np.array([-25.1613186, 0.20623159, 0.20147149])\n",
    "X = x_test\n",
    "predictions = predict( X,theta)\n",
    "print(predictions)\n",
    "correct = []\n",
    "for i in range(len(predictions)):\n",
    "    if predictions[i] == y_test[i]:\n",
    "        correct.append(1)\n",
    "    else:\n",
    "        correct.append(0)\n",
    "        pass\n",
    "    pass\n",
    "# correct = [1 if a == b else 0 for (a, b) in zip(predictions, y)] 这样也可以返回一个list\n",
    "accuracy = sum(correct)/len(X)\n",
    "print(accuracy*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5ccb65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7cc9e843",
   "metadata": {},
   "source": [
    "# 加正则化项"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "64b7a65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Regularization_Parameter = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "043ed235",
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算偏差值\n",
    "def hypothesis(z):\n",
    "    return sigmoid(z);# 3)Gradient Fuc\n",
    "'''\n",
    "batch gradient descent\n",
    "转为向量化计算\n",
    "'''\n",
    "\n",
    "def computeCostLogisReg(X,y,theta,regular):\n",
    "    # 求偏差\n",
    "    m = len(y) \n",
    "    inner = y.T.dot(np.log(sigmoid(X.dot(theta))))+ (1-y).T.dot(np.log(1-sigmoid(X.dot(theta))))\n",
    "    inner = inner + Regularization_Parameter*sum(np.power(theta, 2))\n",
    "    return  -1*np.sum(inner)/m;\n",
    "\n",
    "def gradient(X, y,theta):\n",
    "    # the gradient of the cost is a vector of the same length as θ where the jth element (for j = 0, 1, . . . , n)\n",
    "    return (X.T @ (hypothesis(X @ theta) - y)) / len(X)\n",
    "# @相当于.dot()\n",
    "\n",
    "def gradientDescentReg(X, y, theta, learning_rate, num_iters, tol,print_step,Regularization_Parameter):\n",
    "    #参数说明: X数据, y预测目标值, theta参数,learinig_rate 学习率  num_iters迭代次数 tol精度(0.01)\n",
    "    m = len(y)\n",
    "    J_history = np.zeros((num_iters, 1))\n",
    "    Grad_hist = np.zeros((num_iters, 1))\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        n = len(theta)\n",
    "        theta_temp = theta\n",
    "#         print((X.dot(theta)-y).shape)\n",
    "#         print(X[:,1].shape)\n",
    "#         print(X.shape,theta.shape,(X.dot(theta)-y).shape,y.shape,sigmoid(X.dot(theta)-y).shape)\n",
    "        for j in range(n):\n",
    "            theta_temp[j] = theta[j]+ learning_rate*(y-hypothesis(X.dot(theta))).T.dot(X[:,j])\n",
    "#             theta_temp[j] = theta[j] - learning_rate/m*(X.dot(theta)-y).T.dot(X[:,j]); \n",
    "        theta = theta_temp \n",
    "        cost = computeCostLogisReg(X, y, theta, Regularization_Parameter )\n",
    "        J_history[i] = cost\n",
    "        grad = np.sum(gradient(X, y,theta))\n",
    "        Grad_hist[i] = grad\n",
    "        if(i%print_step==0):\n",
    "            print('第%d次迭代, cost = %f grad =%f' %(i,cost,grad))\n",
    "#         if(cost < tol): #如果 cost 小于精确值 则退出\n",
    "        if(grad < tol): #如果 梯度 小于精确值 则退出\n",
    "            print('迭代训练结束,迭代次数:%d, 偏差值cost=%f grad =%f'%(i,cost,grad))\n",
    "            return (theta, i, J_history,Grad_hist)\n",
    "    \n",
    "    return (theta,i,J_history,Grad_hist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "72257e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "X= x_train\n",
    "y = y_train\n",
    "learning_rate = 0.001\n",
    "num_iters = 20000\n",
    "tol = 0.0001\n",
    "print_step = 50\n",
    "theta = np.zeros((x_cols,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c085924c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0次迭代, cost = 0.590188 grad =1.398392\n",
      "第50次迭代, cost = 0.433957 grad =0.035193\n",
      "第100次迭代, cost = 0.416301 grad =0.024122\n",
      "第150次迭代, cost = 0.407285 grad =0.018517\n",
      "第200次迭代, cost = 0.401191 grad =0.015051\n",
      "第250次迭代, cost = 0.396546 grad =0.012661\n",
      "第300次迭代, cost = 0.392767 grad =0.010892\n",
      "第350次迭代, cost = 0.389565 grad =0.009515\n",
      "第400次迭代, cost = 0.386771 grad =0.008408\n",
      "第450次迭代, cost = 0.384281 grad =0.007496\n",
      "第500次迭代, cost = 0.382026 grad =0.006734\n",
      "第550次迭代, cost = 0.379956 grad =0.006087\n",
      "第600次迭代, cost = 0.378037 grad =0.005535\n",
      "第650次迭代, cost = 0.376242 grad =0.005059\n",
      "第700次迭代, cost = 0.374552 grad =0.004647\n",
      "第750次迭代, cost = 0.372950 grad =0.004288\n",
      "第800次迭代, cost = 0.371426 grad =0.003973\n",
      "第850次迭代, cost = 0.369968 grad =0.003697\n",
      "第900次迭代, cost = 0.368569 grad =0.003454\n",
      "第950次迭代, cost = 0.367222 grad =0.003238\n",
      "第1000次迭代, cost = 0.365923 grad =0.003046\n",
      "第1050次迭代, cost = 0.364666 grad =0.002875\n",
      "第1100次迭代, cost = 0.363447 grad =0.002723\n",
      "第1150次迭代, cost = 0.362264 grad =0.002586\n",
      "第1200次迭代, cost = 0.361113 grad =0.002462\n",
      "第1250次迭代, cost = 0.359992 grad =0.002351\n",
      "第1300次迭代, cost = 0.358900 grad =0.002251\n",
      "第1350次迭代, cost = 0.357834 grad =0.002160\n",
      "第1400次迭代, cost = 0.356792 grad =0.002078\n",
      "第1450次迭代, cost = 0.355773 grad =0.002002\n",
      "第1500次迭代, cost = 0.354776 grad =0.001934\n",
      "第1550次迭代, cost = 0.353800 grad =0.001871\n",
      "第1600次迭代, cost = 0.352843 grad =0.001813\n",
      "第1650次迭代, cost = 0.351906 grad =0.001761\n",
      "第1700次迭代, cost = 0.350985 grad =0.001712\n",
      "第1750次迭代, cost = 0.350082 grad =0.001667\n",
      "第1800次迭代, cost = 0.349196 grad =0.001626\n",
      "第1850次迭代, cost = 0.348324 grad =0.001588\n",
      "第1900次迭代, cost = 0.347468 grad =0.001552\n",
      "第1950次迭代, cost = 0.346626 grad =0.001519\n",
      "第2000次迭代, cost = 0.345799 grad =0.001489\n",
      "第2050次迭代, cost = 0.344984 grad =0.001460\n",
      "第2100次迭代, cost = 0.344182 grad =0.001434\n",
      "第2150次迭代, cost = 0.343393 grad =0.001409\n",
      "第2200次迭代, cost = 0.342616 grad =0.001386\n",
      "第2250次迭代, cost = 0.341850 grad =0.001364\n",
      "第2300次迭代, cost = 0.341096 grad =0.001344\n",
      "第2350次迭代, cost = 0.340353 grad =0.001325\n",
      "第2400次迭代, cost = 0.339620 grad =0.001307\n",
      "第2450次迭代, cost = 0.338897 grad =0.001290\n",
      "第2500次迭代, cost = 0.338184 grad =0.001274\n",
      "第2550次迭代, cost = 0.337480 grad =0.001259\n",
      "第2600次迭代, cost = 0.336786 grad =0.001244\n",
      "第2650次迭代, cost = 0.336101 grad =0.001231\n",
      "第2700次迭代, cost = 0.335425 grad =0.001218\n",
      "第2750次迭代, cost = 0.334757 grad =0.001206\n",
      "第2800次迭代, cost = 0.334098 grad =0.001194\n",
      "第2850次迭代, cost = 0.333446 grad =0.001183\n",
      "第2900次迭代, cost = 0.332803 grad =0.001172\n",
      "第2950次迭代, cost = 0.332166 grad =0.001162\n",
      "第3000次迭代, cost = 0.331538 grad =0.001152\n",
      "第3050次迭代, cost = 0.330916 grad =0.001143\n",
      "第3100次迭代, cost = 0.330301 grad =0.001134\n",
      "第3150次迭代, cost = 0.329694 grad =0.001125\n",
      "第3200次迭代, cost = 0.329092 grad =0.001116\n",
      "第3250次迭代, cost = 0.328498 grad =0.001108\n",
      "第3300次迭代, cost = 0.327909 grad =0.001101\n",
      "第3350次迭代, cost = 0.327327 grad =0.001093\n",
      "第3400次迭代, cost = 0.326750 grad =0.001086\n",
      "第3450次迭代, cost = 0.326180 grad =0.001079\n",
      "第3500次迭代, cost = 0.325615 grad =0.001072\n",
      "第3550次迭代, cost = 0.325055 grad =0.001065\n",
      "第3600次迭代, cost = 0.324501 grad =0.001058\n",
      "第3650次迭代, cost = 0.323952 grad =0.001052\n",
      "第3700次迭代, cost = 0.323408 grad =0.001046\n",
      "第3750次迭代, cost = 0.322869 grad =0.001040\n",
      "第3800次迭代, cost = 0.322335 grad =0.001034\n",
      "第3850次迭代, cost = 0.321805 grad =0.001028\n",
      "第3900次迭代, cost = 0.321280 grad =0.001022\n",
      "第3950次迭代, cost = 0.320760 grad =0.001016\n",
      "第4000次迭代, cost = 0.320244 grad =0.001011\n",
      "第4050次迭代, cost = 0.319732 grad =0.001005\n",
      "第4100次迭代, cost = 0.319224 grad =0.001000\n",
      "第4150次迭代, cost = 0.318721 grad =0.000995\n",
      "第4200次迭代, cost = 0.318221 grad =0.000990\n",
      "第4250次迭代, cost = 0.317725 grad =0.000985\n",
      "第4300次迭代, cost = 0.317233 grad =0.000980\n",
      "第4350次迭代, cost = 0.316745 grad =0.000975\n",
      "第4400次迭代, cost = 0.316260 grad =0.000970\n",
      "第4450次迭代, cost = 0.315779 grad =0.000965\n",
      "第4500次迭代, cost = 0.315301 grad =0.000960\n",
      "第4550次迭代, cost = 0.314827 grad =0.000956\n",
      "第4600次迭代, cost = 0.314356 grad =0.000951\n",
      "第4650次迭代, cost = 0.313887 grad =0.000946\n",
      "第4700次迭代, cost = 0.313423 grad =0.000942\n",
      "第4750次迭代, cost = 0.312961 grad =0.000937\n",
      "第4800次迭代, cost = 0.312502 grad =0.000933\n",
      "第4850次迭代, cost = 0.312046 grad =0.000928\n",
      "第4900次迭代, cost = 0.311592 grad =0.000924\n",
      "第4950次迭代, cost = 0.311142 grad =0.000920\n",
      "第5000次迭代, cost = 0.310694 grad =0.000916\n",
      "第5050次迭代, cost = 0.310249 grad =0.000911\n",
      "第5100次迭代, cost = 0.309806 grad =0.000907\n",
      "第5150次迭代, cost = 0.309366 grad =0.000903\n",
      "第5200次迭代, cost = 0.308928 grad =0.000899\n",
      "第5250次迭代, cost = 0.308493 grad =0.000895\n",
      "第5300次迭代, cost = 0.308060 grad =0.000891\n",
      "第5350次迭代, cost = 0.307630 grad =0.000887\n",
      "第5400次迭代, cost = 0.307201 grad =0.000883\n",
      "第5450次迭代, cost = 0.306775 grad =0.000879\n",
      "第5500次迭代, cost = 0.306351 grad =0.000875\n",
      "第5550次迭代, cost = 0.305929 grad =0.000871\n",
      "第5600次迭代, cost = 0.305509 grad =0.000867\n",
      "第5650次迭代, cost = 0.305091 grad =0.000863\n",
      "第5700次迭代, cost = 0.304675 grad =0.000860\n",
      "第5750次迭代, cost = 0.304261 grad =0.000856\n",
      "第5800次迭代, cost = 0.303849 grad =0.000852\n",
      "第5850次迭代, cost = 0.303439 grad =0.000849\n",
      "第5900次迭代, cost = 0.303030 grad =0.000845\n",
      "第5950次迭代, cost = 0.302624 grad =0.000841\n",
      "第6000次迭代, cost = 0.302219 grad =0.000838\n",
      "第6050次迭代, cost = 0.301815 grad =0.000834\n",
      "第6100次迭代, cost = 0.301414 grad =0.000831\n",
      "第6150次迭代, cost = 0.301013 grad =0.000827\n",
      "第6200次迭代, cost = 0.300615 grad =0.000824\n",
      "第6250次迭代, cost = 0.300218 grad =0.000820\n",
      "第6300次迭代, cost = 0.299823 grad =0.000817\n",
      "第6350次迭代, cost = 0.299429 grad =0.000813\n",
      "第6400次迭代, cost = 0.299036 grad =0.000810\n",
      "第6450次迭代, cost = 0.298645 grad =0.000807\n",
      "第6500次迭代, cost = 0.298255 grad =0.000803\n",
      "第6550次迭代, cost = 0.297867 grad =0.000800\n",
      "第6600次迭代, cost = 0.297480 grad =0.000797\n",
      "第6650次迭代, cost = 0.297094 grad =0.000793\n",
      "第6700次迭代, cost = 0.296710 grad =0.000790\n",
      "第6750次迭代, cost = 0.296327 grad =0.000787\n",
      "第6800次迭代, cost = 0.295945 grad =0.000784\n",
      "第6850次迭代, cost = 0.295564 grad =0.000781\n",
      "第6900次迭代, cost = 0.295185 grad =0.000778\n",
      "第6950次迭代, cost = 0.294806 grad =0.000774\n",
      "第7000次迭代, cost = 0.294429 grad =0.000771\n",
      "第7050次迭代, cost = 0.294053 grad =0.000768\n",
      "第7100次迭代, cost = 0.293678 grad =0.000765\n",
      "第7150次迭代, cost = 0.293304 grad =0.000762\n",
      "第7200次迭代, cost = 0.292931 grad =0.000759\n",
      "第7250次迭代, cost = 0.292560 grad =0.000756\n",
      "第7300次迭代, cost = 0.292189 grad =0.000753\n",
      "第7350次迭代, cost = 0.291819 grad =0.000751\n",
      "第7400次迭代, cost = 0.291450 grad =0.000748\n",
      "第7450次迭代, cost = 0.291082 grad =0.000745\n",
      "第7500次迭代, cost = 0.290715 grad =0.000742\n",
      "第7550次迭代, cost = 0.290349 grad =0.000739\n",
      "第7600次迭代, cost = 0.289984 grad =0.000736\n",
      "第7650次迭代, cost = 0.289620 grad =0.000734\n",
      "第7700次迭代, cost = 0.289257 grad =0.000731\n",
      "第7750次迭代, cost = 0.288894 grad =0.000728\n",
      "第7800次迭代, cost = 0.288533 grad =0.000725\n",
      "第7850次迭代, cost = 0.288172 grad =0.000723\n",
      "第7900次迭代, cost = 0.287812 grad =0.000720\n",
      "第7950次迭代, cost = 0.287453 grad =0.000717\n",
      "第8000次迭代, cost = 0.287095 grad =0.000715\n",
      "第8050次迭代, cost = 0.286737 grad =0.000712\n",
      "第8100次迭代, cost = 0.286380 grad =0.000710\n",
      "第8150次迭代, cost = 0.286024 grad =0.000707\n",
      "第8200次迭代, cost = 0.285669 grad =0.000704\n",
      "第8250次迭代, cost = 0.285314 grad =0.000702\n",
      "第8300次迭代, cost = 0.284961 grad =0.000699\n",
      "第8350次迭代, cost = 0.284607 grad =0.000697\n",
      "第8400次迭代, cost = 0.284255 grad =0.000694\n",
      "第8450次迭代, cost = 0.283903 grad =0.000692\n",
      "第8500次迭代, cost = 0.283552 grad =0.000690\n",
      "第8550次迭代, cost = 0.283202 grad =0.000687\n",
      "第8600次迭代, cost = 0.282852 grad =0.000685\n",
      "第8650次迭代, cost = 0.282503 grad =0.000682\n",
      "第8700次迭代, cost = 0.282154 grad =0.000680\n",
      "第8750次迭代, cost = 0.281806 grad =0.000678\n",
      "第8800次迭代, cost = 0.281459 grad =0.000675\n",
      "第8850次迭代, cost = 0.281112 grad =0.000673\n",
      "第8900次迭代, cost = 0.280766 grad =0.000671\n",
      "第8950次迭代, cost = 0.280421 grad =0.000668\n",
      "第9000次迭代, cost = 0.280076 grad =0.000666\n",
      "第9050次迭代, cost = 0.279731 grad =0.000664\n",
      "第9100次迭代, cost = 0.279388 grad =0.000662\n",
      "第9150次迭代, cost = 0.279044 grad =0.000660\n",
      "第9200次迭代, cost = 0.278702 grad =0.000657\n",
      "第9250次迭代, cost = 0.278359 grad =0.000655\n",
      "第9300次迭代, cost = 0.278018 grad =0.000653\n",
      "第9350次迭代, cost = 0.277677 grad =0.000651\n",
      "第9400次迭代, cost = 0.277336 grad =0.000649\n",
      "第9450次迭代, cost = 0.276996 grad =0.000647\n",
      "第9500次迭代, cost = 0.276656 grad =0.000644\n",
      "第9550次迭代, cost = 0.276317 grad =0.000642\n",
      "第9600次迭代, cost = 0.275979 grad =0.000640\n",
      "第9650次迭代, cost = 0.275641 grad =0.000638\n",
      "第9700次迭代, cost = 0.275303 grad =0.000636\n",
      "第9750次迭代, cost = 0.274966 grad =0.000634\n",
      "第9800次迭代, cost = 0.274629 grad =0.000632\n",
      "第9850次迭代, cost = 0.274293 grad =0.000630\n",
      "第9900次迭代, cost = 0.273957 grad =0.000628\n",
      "第9950次迭代, cost = 0.273622 grad =0.000626\n",
      "第10000次迭代, cost = 0.273287 grad =0.000624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第10050次迭代, cost = 0.272952 grad =0.000622\n",
      "第10100次迭代, cost = 0.272618 grad =0.000620\n",
      "第10150次迭代, cost = 0.272285 grad =0.000618\n",
      "第10200次迭代, cost = 0.271952 grad =0.000617\n",
      "第10250次迭代, cost = 0.271619 grad =0.000615\n",
      "第10300次迭代, cost = 0.271286 grad =0.000613\n",
      "第10350次迭代, cost = 0.270955 grad =0.000611\n",
      "第10400次迭代, cost = 0.270623 grad =0.000609\n",
      "第10450次迭代, cost = 0.270292 grad =0.000607\n",
      "第10500次迭代, cost = 0.269961 grad =0.000605\n",
      "第10550次迭代, cost = 0.269631 grad =0.000604\n",
      "第10600次迭代, cost = 0.269301 grad =0.000602\n",
      "第10650次迭代, cost = 0.268971 grad =0.000600\n",
      "第10700次迭代, cost = 0.268642 grad =0.000598\n",
      "第10750次迭代, cost = 0.268313 grad =0.000596\n",
      "第10800次迭代, cost = 0.267985 grad =0.000595\n",
      "第10850次迭代, cost = 0.267657 grad =0.000593\n",
      "第10900次迭代, cost = 0.267329 grad =0.000591\n",
      "第10950次迭代, cost = 0.267002 grad =0.000589\n",
      "第11000次迭代, cost = 0.266675 grad =0.000588\n",
      "第11050次迭代, cost = 0.266348 grad =0.000586\n",
      "第11100次迭代, cost = 0.266022 grad =0.000584\n",
      "第11150次迭代, cost = 0.265696 grad =0.000583\n",
      "第11200次迭代, cost = 0.265370 grad =0.000581\n",
      "第11250次迭代, cost = 0.265045 grad =0.000579\n",
      "第11300次迭代, cost = 0.264720 grad =0.000578\n",
      "第11350次迭代, cost = 0.264395 grad =0.000576\n",
      "第11400次迭代, cost = 0.264071 grad =0.000574\n",
      "第11450次迭代, cost = 0.263747 grad =0.000573\n",
      "第11500次迭代, cost = 0.263423 grad =0.000571\n",
      "第11550次迭代, cost = 0.263100 grad =0.000570\n",
      "第11600次迭代, cost = 0.262777 grad =0.000568\n",
      "第11650次迭代, cost = 0.262454 grad =0.000566\n",
      "第11700次迭代, cost = 0.262132 grad =0.000565\n",
      "第11750次迭代, cost = 0.261810 grad =0.000563\n",
      "第11800次迭代, cost = 0.261488 grad =0.000562\n",
      "第11850次迭代, cost = 0.261167 grad =0.000560\n",
      "第11900次迭代, cost = 0.260846 grad =0.000559\n",
      "第11950次迭代, cost = 0.260525 grad =0.000557\n",
      "第12000次迭代, cost = 0.260204 grad =0.000556\n",
      "第12050次迭代, cost = 0.259884 grad =0.000554\n",
      "第12100次迭代, cost = 0.259564 grad =0.000553\n",
      "第12150次迭代, cost = 0.259245 grad =0.000551\n",
      "第12200次迭代, cost = 0.258925 grad =0.000550\n",
      "第12250次迭代, cost = 0.258606 grad =0.000548\n",
      "第12300次迭代, cost = 0.258288 grad =0.000547\n",
      "第12350次迭代, cost = 0.257969 grad =0.000545\n",
      "第12400次迭代, cost = 0.257651 grad =0.000544\n",
      "第12450次迭代, cost = 0.257333 grad =0.000542\n",
      "第12500次迭代, cost = 0.257015 grad =0.000541\n",
      "第12550次迭代, cost = 0.256698 grad =0.000539\n",
      "第12600次迭代, cost = 0.256381 grad =0.000538\n",
      "第12650次迭代, cost = 0.256064 grad =0.000537\n",
      "第12700次迭代, cost = 0.255748 grad =0.000535\n",
      "第12750次迭代, cost = 0.255431 grad =0.000534\n",
      "第12800次迭代, cost = 0.255115 grad =0.000532\n",
      "第12850次迭代, cost = 0.254800 grad =0.000531\n",
      "第12900次迭代, cost = 0.254484 grad =0.000530\n",
      "第12950次迭代, cost = 0.254169 grad =0.000528\n",
      "第13000次迭代, cost = 0.253854 grad =0.000527\n",
      "第13050次迭代, cost = 0.253539 grad =0.000525\n",
      "第13100次迭代, cost = 0.253225 grad =0.000524\n",
      "第13150次迭代, cost = 0.252911 grad =0.000523\n",
      "第13200次迭代, cost = 0.252597 grad =0.000521\n",
      "第13250次迭代, cost = 0.252283 grad =0.000520\n",
      "第13300次迭代, cost = 0.251970 grad =0.000519\n",
      "第13350次迭代, cost = 0.251656 grad =0.000517\n",
      "第13400次迭代, cost = 0.251343 grad =0.000516\n",
      "第13450次迭代, cost = 0.251031 grad =0.000515\n",
      "第13500次迭代, cost = 0.250718 grad =0.000514\n",
      "第13550次迭代, cost = 0.250406 grad =0.000512\n",
      "第13600次迭代, cost = 0.250094 grad =0.000511\n",
      "第13650次迭代, cost = 0.249783 grad =0.000510\n",
      "第13700次迭代, cost = 0.249471 grad =0.000508\n",
      "第13750次迭代, cost = 0.249160 grad =0.000507\n",
      "第13800次迭代, cost = 0.248849 grad =0.000506\n",
      "第13850次迭代, cost = 0.248538 grad =0.000505\n",
      "第13900次迭代, cost = 0.248228 grad =0.000503\n",
      "第13950次迭代, cost = 0.247917 grad =0.000502\n",
      "第14000次迭代, cost = 0.247607 grad =0.000501\n",
      "第14050次迭代, cost = 0.247297 grad =0.000500\n",
      "第14100次迭代, cost = 0.246988 grad =0.000498\n",
      "第14150次迭代, cost = 0.246679 grad =0.000497\n",
      "第14200次迭代, cost = 0.246369 grad =0.000496\n",
      "第14250次迭代, cost = 0.246061 grad =0.000495\n",
      "第14300次迭代, cost = 0.245752 grad =0.000494\n",
      "第14350次迭代, cost = 0.245443 grad =0.000492\n",
      "第14400次迭代, cost = 0.245135 grad =0.000491\n",
      "第14450次迭代, cost = 0.244827 grad =0.000490\n",
      "第14500次迭代, cost = 0.244519 grad =0.000489\n",
      "第14550次迭代, cost = 0.244212 grad =0.000488\n",
      "第14600次迭代, cost = 0.243905 grad =0.000487\n",
      "第14650次迭代, cost = 0.243597 grad =0.000485\n",
      "第14700次迭代, cost = 0.243291 grad =0.000484\n",
      "第14750次迭代, cost = 0.242984 grad =0.000483\n",
      "第14800次迭代, cost = 0.242677 grad =0.000482\n",
      "第14850次迭代, cost = 0.242371 grad =0.000481\n",
      "第14900次迭代, cost = 0.242065 grad =0.000480\n",
      "第14950次迭代, cost = 0.241759 grad =0.000479\n",
      "第15000次迭代, cost = 0.241454 grad =0.000477\n",
      "第15050次迭代, cost = 0.241148 grad =0.000476\n",
      "第15100次迭代, cost = 0.240843 grad =0.000475\n",
      "第15150次迭代, cost = 0.240538 grad =0.000474\n",
      "第15200次迭代, cost = 0.240233 grad =0.000473\n",
      "第15250次迭代, cost = 0.239929 grad =0.000472\n",
      "第15300次迭代, cost = 0.239624 grad =0.000471\n",
      "第15350次迭代, cost = 0.239320 grad =0.000470\n",
      "第15400次迭代, cost = 0.239016 grad =0.000469\n",
      "第15450次迭代, cost = 0.238713 grad =0.000468\n",
      "第15500次迭代, cost = 0.238409 grad =0.000466\n",
      "第15550次迭代, cost = 0.238106 grad =0.000465\n",
      "第15600次迭代, cost = 0.237803 grad =0.000464\n",
      "第15650次迭代, cost = 0.237500 grad =0.000463\n",
      "第15700次迭代, cost = 0.237197 grad =0.000462\n",
      "第15750次迭代, cost = 0.236894 grad =0.000461\n",
      "第15800次迭代, cost = 0.236592 grad =0.000460\n",
      "第15850次迭代, cost = 0.236290 grad =0.000459\n",
      "第15900次迭代, cost = 0.235988 grad =0.000458\n",
      "第15950次迭代, cost = 0.235686 grad =0.000457\n",
      "第16000次迭代, cost = 0.235385 grad =0.000456\n",
      "第16050次迭代, cost = 0.235083 grad =0.000455\n",
      "第16100次迭代, cost = 0.234782 grad =0.000454\n",
      "第16150次迭代, cost = 0.234481 grad =0.000453\n",
      "第16200次迭代, cost = 0.234181 grad =0.000452\n",
      "第16250次迭代, cost = 0.233880 grad =0.000451\n",
      "第16300次迭代, cost = 0.233580 grad =0.000450\n",
      "第16350次迭代, cost = 0.233279 grad =0.000449\n",
      "第16400次迭代, cost = 0.232979 grad =0.000448\n",
      "第16450次迭代, cost = 0.232680 grad =0.000447\n",
      "第16500次迭代, cost = 0.232380 grad =0.000446\n",
      "第16550次迭代, cost = 0.232081 grad =0.000445\n",
      "第16600次迭代, cost = 0.231781 grad =0.000444\n",
      "第16650次迭代, cost = 0.231482 grad =0.000443\n",
      "第16700次迭代, cost = 0.231184 grad =0.000442\n",
      "第16750次迭代, cost = 0.230885 grad =0.000441\n",
      "第16800次迭代, cost = 0.230586 grad =0.000440\n",
      "第16850次迭代, cost = 0.230288 grad =0.000439\n",
      "第16900次迭代, cost = 0.229990 grad =0.000438\n",
      "第16950次迭代, cost = 0.229692 grad =0.000437\n",
      "第17000次迭代, cost = 0.229394 grad =0.000436\n",
      "第17050次迭代, cost = 0.229097 grad =0.000435\n",
      "第17100次迭代, cost = 0.228799 grad =0.000434\n",
      "第17150次迭代, cost = 0.228502 grad =0.000433\n",
      "第17200次迭代, cost = 0.228205 grad =0.000432\n",
      "第17250次迭代, cost = 0.227908 grad =0.000431\n",
      "第17300次迭代, cost = 0.227612 grad =0.000430\n",
      "第17350次迭代, cost = 0.227315 grad =0.000429\n",
      "第17400次迭代, cost = 0.227019 grad =0.000429\n",
      "第17450次迭代, cost = 0.226723 grad =0.000428\n",
      "第17500次迭代, cost = 0.226427 grad =0.000427\n",
      "第17550次迭代, cost = 0.226131 grad =0.000426\n",
      "第17600次迭代, cost = 0.225836 grad =0.000425\n",
      "第17650次迭代, cost = 0.225540 grad =0.000424\n",
      "第17700次迭代, cost = 0.225245 grad =0.000423\n",
      "第17750次迭代, cost = 0.224950 grad =0.000422\n",
      "第17800次迭代, cost = 0.224655 grad =0.000421\n",
      "第17850次迭代, cost = 0.224360 grad =0.000420\n",
      "第17900次迭代, cost = 0.224066 grad =0.000419\n",
      "第17950次迭代, cost = 0.223771 grad =0.000419\n",
      "第18000次迭代, cost = 0.223477 grad =0.000418\n",
      "第18050次迭代, cost = 0.223183 grad =0.000417\n",
      "第18100次迭代, cost = 0.222889 grad =0.000416\n",
      "第18150次迭代, cost = 0.222596 grad =0.000415\n",
      "第18200次迭代, cost = 0.222302 grad =0.000414\n",
      "第18250次迭代, cost = 0.222009 grad =0.000413\n",
      "第18300次迭代, cost = 0.221716 grad =0.000412\n",
      "第18350次迭代, cost = 0.221423 grad =0.000412\n",
      "第18400次迭代, cost = 0.221130 grad =0.000411\n",
      "第18450次迭代, cost = 0.220837 grad =0.000410\n",
      "第18500次迭代, cost = 0.220545 grad =0.000409\n",
      "第18550次迭代, cost = 0.220253 grad =0.000408\n",
      "第18600次迭代, cost = 0.219961 grad =0.000407\n",
      "第18650次迭代, cost = 0.219669 grad =0.000406\n",
      "第18700次迭代, cost = 0.219377 grad =0.000406\n",
      "第18750次迭代, cost = 0.219085 grad =0.000405\n",
      "第18800次迭代, cost = 0.218794 grad =0.000404\n",
      "第18850次迭代, cost = 0.218502 grad =0.000403\n",
      "第18900次迭代, cost = 0.218211 grad =0.000402\n",
      "第18950次迭代, cost = 0.217920 grad =0.000401\n",
      "第19000次迭代, cost = 0.217630 grad =0.000401\n",
      "第19050次迭代, cost = 0.217339 grad =0.000400\n",
      "第19100次迭代, cost = 0.217048 grad =0.000399\n",
      "第19150次迭代, cost = 0.216758 grad =0.000398\n",
      "第19200次迭代, cost = 0.216468 grad =0.000397\n",
      "第19250次迭代, cost = 0.216178 grad =0.000397\n",
      "第19300次迭代, cost = 0.215888 grad =0.000396\n",
      "第19350次迭代, cost = 0.215599 grad =0.000395\n",
      "第19400次迭代, cost = 0.215309 grad =0.000394\n",
      "第19450次迭代, cost = 0.215020 grad =0.000393\n",
      "第19500次迭代, cost = 0.214731 grad =0.000393\n",
      "第19550次迭代, cost = 0.214442 grad =0.000392\n",
      "第19600次迭代, cost = 0.214153 grad =0.000391\n",
      "第19650次迭代, cost = 0.213864 grad =0.000390\n",
      "第19700次迭代, cost = 0.213576 grad =0.000389\n",
      "第19750次迭代, cost = 0.213287 grad =0.000389\n",
      "第19800次迭代, cost = 0.212999 grad =0.000388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第19850次迭代, cost = 0.212711 grad =0.000387\n",
      "第19900次迭代, cost = 0.212423 grad =0.000386\n",
      "第19950次迭代, cost = 0.212135 grad =0.000385\n"
     ]
    }
   ],
   "source": [
    "(theta,iters,J_hist,Grad_hist) = gradientDescentReg(X, y, theta, learning_rate, num_iters, tol,print_step,Regularization_Parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "44db7cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "73.6\n"
     ]
    }
   ],
   "source": [
    "# learning_parameters = np.array([-25.1613186, 0.20623159, 0.20147149])\n",
    "X = x_test\n",
    "predictions = predict( X,theta)\n",
    "print(predictions)\n",
    "correct = []\n",
    "for i in range(len(predictions)):\n",
    "    if predictions[i] == y_test[i]:\n",
    "        correct.append(1)\n",
    "    else:\n",
    "        correct.append(0)\n",
    "        pass\n",
    "    pass\n",
    "# correct = [1 if a == b else 0 for (a, b) in zip(predictions, y)] 这样也可以返回一个list\n",
    "accuracy = sum(correct)/len(X)\n",
    "print(accuracy*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb94006b",
   "metadata": {},
   "source": [
    "#### 加了正则项, 正确率变高了1 cost减少了诶!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6740c975",
   "metadata": {},
   "source": [
    "2.\t对数据的某一列乘上一个很大的数（比如100000），再分别调用梯度下降法与牛顿法计算Logistic回归模型的参数，你有什么发现？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bd275ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0次迭代, cost = nan grad =-7802.958847\n",
      "迭代训练结束,迭代次数:0, 偏差值cost=nan grad =-7802.958847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sjc\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  \n",
      "C:\\Users\\sjc\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:12: RuntimeWarning: divide by zero encountered in log\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "A1 = np.array(x_train, copy=True)\n",
    "A1[:,2] = A1[:,2]*100000\n",
    "\n",
    "X= A1\n",
    "y = y_train\n",
    "learning_rate = 0.001\n",
    "num_iters = 20000\n",
    "tol = 0.0001\n",
    "print_step = 50\n",
    "theta = np.zeros((x_cols,1))\n",
    "(theta,iters,J_hist,Grad_hist) = gradientDescent(X, y, theta, learning_rate, num_iters, tol,print_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fd484f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1206805ac08>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(J_history.size)\n",
    "\n",
    "x_axis = np.linspace(0,iters,iters)\n",
    "plt.plot(x_axis,J_history[0:iters],'+')\n",
    "plt.plot(x_axis,Grad_hist[0:iters],'.')\n",
    "plt.legend(['cost','grad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "32876ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0次迭代, cost = 0.692217 grad =16905.632030\n",
      "第50次迭代, cost = 0.650031 grad =15295.448130\n",
      "第100次迭代, cost = 0.614932 grad =13838.817742\n",
      "第150次迭代, cost = 0.585352 grad =12521.038841\n",
      "第200次迭代, cost = 0.560174 grad =11328.836328\n",
      "第250次迭代, cost = 0.538573 grad =10250.216438\n",
      "第300次迭代, cost = 0.519927 grad =9274.338884\n",
      "第350次迭代, cost = 0.503751 grad =8391.403347\n",
      "第400次迭代, cost = 0.489666 grad =7592.548073\n",
      "第450次迭代, cost = 0.477363 grad =6869.758905\n",
      "第500次迭代, cost = 0.466594 grad =6215.787484\n",
      "第550次迭代, cost = 0.457153 grad =5624.077568\n",
      "第600次迭代, cost = 0.448866 grad =5088.698582\n",
      "第650次迭代, cost = 0.441589 grad =4604.285653\n",
      "第700次迭代, cost = 0.435197 grad =4165.985455\n",
      "第750次迭代, cost = 0.429583 grad =3769.407289\n",
      "第800次迭代, cost = 0.424656 grad =3410.578876\n",
      "第850次迭代, cost = 0.420333 grad =3085.906396\n",
      "第900次迭代, cost = 0.416544 grad =2792.138368\n",
      "第950次迭代, cost = 0.413226 grad =2526.332984\n",
      "第1000次迭代, cost = 0.410325 grad =2285.828580\n",
      "第1050次迭代, cost = 0.407790 grad =2068.216924\n",
      "第1100次迭代, cost = 0.405578 grad =1871.319062\n",
      "第1150次迭代, cost = 0.403651 grad =1693.163467\n",
      "第1200次迭代, cost = 0.401974 grad =1531.966282\n",
      "第1250次迭代, cost = 0.400516 grad =1386.113444\n",
      "第1300次迭代, cost = 0.399251 grad =1254.144510\n",
      "第1350次迭代, cost = 0.398153 grad =1134.738037\n",
      "第1400次迭代, cost = 0.397203 grad =1026.698346\n",
      "第1450次迭代, cost = 0.396381 grad =928.943549\n",
      "第1500次迭代, cost = 0.395670 grad =840.494726\n",
      "第1550次迭代, cost = 0.395056 grad =760.466118\n",
      "第1600次迭代, cost = 0.394527 grad =688.056267\n",
      "第1650次迭代, cost = 0.394071 grad =622.539990\n",
      "第1700次迭代, cost = 0.393679 grad =563.261123\n",
      "第1750次迭代, cost = 0.393342 grad =509.625949\n",
      "第1800次迭代, cost = 0.393052 grad =461.097258\n",
      "第1850次迭代, cost = 0.392803 grad =417.188969\n",
      "第1900次迭代, cost = 0.392589 grad =377.461261\n",
      "第1950次迭代, cost = 0.392406 grad =341.516173\n",
      "第2000次迭代, cost = 0.392249 grad =308.993620\n",
      "第2050次迭代, cost = 0.392114 grad =279.567785\n",
      "第2100次迭代, cost = 0.391999 grad =252.943864\n",
      "第2150次迭代, cost = 0.391900 grad =228.855109\n",
      "第2200次迭代, cost = 0.391815 grad =207.060162\n",
      "第2250次迭代, cost = 0.391743 grad =187.340638\n",
      "第2300次迭代, cost = 0.391680 grad =169.498936\n",
      "第2350次迭代, cost = 0.391626 grad =153.356268\n",
      "第2400次迭代, cost = 0.391580 grad =138.750860\n",
      "第2450次迭代, cost = 0.391541 grad =125.536341\n",
      "第2500次迭代, cost = 0.391506 grad =113.580272\n",
      "第2550次迭代, cost = 0.391477 grad =102.762823\n",
      "第2600次迭代, cost = 0.391451 grad =92.975572\n",
      "第2650次迭代, cost = 0.391429 grad =84.120418\n",
      "第2700次迭代, cost = 0.391409 grad =76.108601\n",
      "第2750次迭代, cost = 0.391393 grad =68.859811\n",
      "第2800次迭代, cost = 0.391378 grad =62.301386\n",
      "第2850次迭代, cost = 0.391365 grad =56.367582\n",
      "第2900次迭代, cost = 0.391354 grad =50.998914\n",
      "第2950次迭代, cost = 0.391344 grad =46.141562\n",
      "第3000次迭代, cost = 0.391336 grad =41.746831\n",
      "第3050次迭代, cost = 0.391328 grad =37.770662\n",
      "第3100次迭代, cost = 0.391321 grad =34.173193\n",
      "第3150次迭代, cost = 0.391316 grad =30.918358\n",
      "第3200次迭代, cost = 0.391310 grad =27.973524\n",
      "第3250次迭代, cost = 0.391306 grad =25.309167\n",
      "第3300次迭代, cost = 0.391302 grad =22.898574\n",
      "第3350次迭代, cost = 0.391298 grad =20.717577\n",
      "第3400次迭代, cost = 0.391295 grad =18.744308\n",
      "第3450次迭代, cost = 0.391292 grad =16.958982\n",
      "第3500次迭代, cost = 0.391289 grad =15.343701\n",
      "第3550次迭代, cost = 0.391287 grad =13.882267\n",
      "第3600次迭代, cost = 0.391285 grad =12.560029\n",
      "第3650次迭代, cost = 0.391283 grad =11.363728\n",
      "第3700次迭代, cost = 0.391282 grad =10.281370\n",
      "第3750次迭代, cost = 0.391280 grad =9.302102\n",
      "第3800次迭代, cost = 0.391279 grad =8.416106\n",
      "第3850次迭代, cost = 0.391278 grad =7.614498\n",
      "第3900次迭代, cost = 0.391276 grad =6.889240\n",
      "第3950次迭代, cost = 0.391276 grad =6.233060\n",
      "第4000次迭代, cost = 0.391275 grad =5.639379\n",
      "第4050次迭代, cost = 0.391274 grad =5.102245\n",
      "第4100次迭代, cost = 0.391273 grad =4.616270\n",
      "第4150次迭代, cost = 0.391273 grad =4.176584\n",
      "第4200次迭代, cost = 0.391272 grad =3.778776\n",
      "第4250次迭代, cost = 0.391272 grad =3.418858\n",
      "第4300次迭代, cost = 0.391271 grad =3.093221\n",
      "第4350次迭代, cost = 0.391271 grad =2.798600\n",
      "第4400次迭代, cost = 0.391270 grad =2.532041\n",
      "第4450次迭代, cost = 0.391270 grad =2.290871\n",
      "第4500次迭代, cost = 0.391270 grad =2.072672\n",
      "第4550次迭代, cost = 0.391269 grad =1.875255\n",
      "第4600次迭代, cost = 0.391269 grad =1.696642\n",
      "第4650次迭代, cost = 0.391269 grad =1.535041\n",
      "第4700次迭代, cost = 0.391269 grad =1.388833\n",
      "第4750次迭代, cost = 0.391268 grad =1.256550\n",
      "第4800次迭代, cost = 0.391268 grad =1.136867\n",
      "第4850次迭代, cost = 0.391268 grad =1.028583\n",
      "第4900次迭代, cost = 0.391268 grad =0.930613\n",
      "第4950次迭代, cost = 0.391268 grad =0.841975\n",
      "第5000次迭代, cost = 0.391268 grad =0.761779\n",
      "第5050次迭代, cost = 0.391268 grad =0.689221\n",
      "第5100次迭代, cost = 0.391268 grad =0.623575\n",
      "第5150次迭代, cost = 0.391268 grad =0.564181\n",
      "第5200次迭代, cost = 0.391267 grad =0.510444\n",
      "第5250次迭代, cost = 0.391267 grad =0.461826\n",
      "第5300次迭代, cost = 0.391267 grad =0.417838\n",
      "第5350次迭代, cost = 0.391267 grad =0.378040\n",
      "第5400次迭代, cost = 0.391267 grad =0.342033\n",
      "第5450次迭代, cost = 0.391267 grad =0.309455\n",
      "第5500次迭代, cost = 0.391267 grad =0.279980\n",
      "第5550次迭代, cost = 0.391267 grad =0.253313\n",
      "第5600次迭代, cost = 0.391267 grad =0.229185\n",
      "第5650次迭代, cost = 0.391267 grad =0.207356\n",
      "第5700次迭代, cost = 0.391267 grad =0.187606\n",
      "第5750次迭代, cost = 0.391267 grad =0.169736\n",
      "第5800次迭代, cost = 0.391267 grad =0.153568\n",
      "第5850次迭代, cost = 0.391267 grad =0.138940\n",
      "第5900次迭代, cost = 0.391267 grad =0.125706\n",
      "第5950次迭代, cost = 0.391267 grad =0.113732\n",
      "第6000次迭代, cost = 0.391267 grad =0.102899\n",
      "第6050次迭代, cost = 0.391267 grad =0.093097\n",
      "第6100次迭代, cost = 0.391267 grad =0.084230\n",
      "第6150次迭代, cost = 0.391267 grad =0.076206\n",
      "第6200次迭代, cost = 0.391267 grad =0.068948\n",
      "第6250次迭代, cost = 0.391267 grad =0.062380\n",
      "第6300次迭代, cost = 0.391267 grad =0.056438\n",
      "第6350次迭代, cost = 0.391267 grad =0.051062\n",
      "第6400次迭代, cost = 0.391267 grad =0.046198\n",
      "第6450次迭代, cost = 0.391267 grad =0.041798\n",
      "第6500次迭代, cost = 0.391267 grad =0.037817\n",
      "第6550次迭代, cost = 0.391267 grad =0.034214\n",
      "第6600次迭代, cost = 0.391267 grad =0.030955\n",
      "第6650次迭代, cost = 0.391267 grad =0.028007\n",
      "第6700次迭代, cost = 0.391267 grad =0.025339\n",
      "第6750次迭代, cost = 0.391267 grad =0.022926\n",
      "第6800次迭代, cost = 0.391267 grad =0.020742\n",
      "第6850次迭代, cost = 0.391267 grad =0.018766\n",
      "第6900次迭代, cost = 0.391267 grad =0.016979\n",
      "第6950次迭代, cost = 0.391267 grad =0.015361\n",
      "第7000次迭代, cost = 0.391267 grad =0.013898\n",
      "第7050次迭代, cost = 0.391267 grad =0.012574\n",
      "第7100次迭代, cost = 0.391267 grad =0.011377\n",
      "第7150次迭代, cost = 0.391267 grad =0.010293\n",
      "第7200次迭代, cost = 0.391267 grad =0.009313\n",
      "第7250次迭代, cost = 0.391267 grad =0.008426\n",
      "第7300次迭代, cost = 0.391267 grad =0.007623\n",
      "第7350次迭代, cost = 0.391267 grad =0.006897\n",
      "第7400次迭代, cost = 0.391267 grad =0.006240\n",
      "第7450次迭代, cost = 0.391267 grad =0.005646\n",
      "第7500次迭代, cost = 0.391267 grad =0.005108\n",
      "第7550次迭代, cost = 0.391267 grad =0.004621\n",
      "第7600次迭代, cost = 0.391267 grad =0.004181\n",
      "第7650次迭代, cost = 0.391267 grad =0.003783\n",
      "第7700次迭代, cost = 0.391267 grad =0.003423\n",
      "第7750次迭代, cost = 0.391267 grad =0.003097\n",
      "第7800次迭代, cost = 0.391267 grad =0.002802\n",
      "第7850次迭代, cost = 0.391267 grad =0.002535\n",
      "第7900次迭代, cost = 0.391267 grad =0.002293\n",
      "第7950次迭代, cost = 0.391267 grad =0.002075\n",
      "第8000次迭代, cost = 0.391267 grad =0.001877\n",
      "第8050次迭代, cost = 0.391267 grad =0.001699\n",
      "第8100次迭代, cost = 0.391267 grad =0.001537\n",
      "第8150次迭代, cost = 0.391267 grad =0.001390\n",
      "第8200次迭代, cost = 0.391267 grad =0.001258\n",
      "第8250次迭代, cost = 0.391267 grad =0.001138\n",
      "第8300次迭代, cost = 0.391267 grad =0.001030\n",
      "第8350次迭代, cost = 0.391267 grad =0.000932\n",
      "第8400次迭代, cost = 0.391267 grad =0.000843\n",
      "第8450次迭代, cost = 0.391267 grad =0.000763\n",
      "第8500次迭代, cost = 0.391267 grad =0.000690\n",
      "第8550次迭代, cost = 0.391267 grad =0.000624\n",
      "第8600次迭代, cost = 0.391267 grad =0.000565\n",
      "第8650次迭代, cost = 0.391267 grad =0.000511\n",
      "第8700次迭代, cost = 0.391267 grad =0.000462\n",
      "第8750次迭代, cost = 0.391267 grad =0.000418\n",
      "第8800次迭代, cost = 0.391267 grad =0.000379\n",
      "第8850次迭代, cost = 0.391267 grad =0.000343\n",
      "第8900次迭代, cost = 0.391267 grad =0.000310\n",
      "第8950次迭代, cost = 0.391267 grad =0.000280\n",
      "第9000次迭代, cost = 0.391267 grad =0.000254\n",
      "第9050次迭代, cost = 0.391267 grad =0.000230\n",
      "第9100次迭代, cost = 0.391267 grad =0.000208\n",
      "第9150次迭代, cost = 0.391267 grad =0.000188\n",
      "第9200次迭代, cost = 0.391267 grad =0.000170\n",
      "第9250次迭代, cost = 0.391267 grad =0.000154\n",
      "第9300次迭代, cost = 0.391267 grad =0.000139\n",
      "第9350次迭代, cost = 0.391267 grad =0.000126\n",
      "第9400次迭代, cost = 0.391267 grad =0.000114\n",
      "第9450次迭代, cost = 0.391267 grad =0.000103\n",
      "迭代训练结束,迭代次数:9466, 偏差值cost=0.391267 grad =0.000100\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "A1 = np.array(x_train, copy=True)\n",
    "A1[:,2] = A1[:,2]*100000\n",
    "\n",
    "X= A1\n",
    "y = y_train\n",
    "learning_rate = 0.001\n",
    "num_iters = 20000\n",
    "tol = 0.0001\n",
    "print_step = 50\n",
    "theta = np.zeros((x_cols,1))\n",
    "(theta,iters,J_history,Grad_hist) = newtonsMethod(X, y, theta, learning_rate, num_iters, tol,print_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fdc25666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n",
      "[0.69221706]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x12068371a88>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(J_history.size)\n",
    "\n",
    "x_axis = np.linspace(0,iters,iters)\n",
    "plt.plot(x_axis,J_history[0:iters],'+')\n",
    "print(J_history[0])\n",
    "plt.plot(x_axis,Grad_hist[0:iters],'.')\n",
    "plt.legend(['cost','grad'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e725229",
   "metadata": {},
   "source": [
    "### cost 比较小图上看不出来 \n",
    "梯度下降好像 不行 应该是值太大了 牛顿法可以计算 不过讲道理, 如果先进行归一化的, 也不会有这种问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f54dd35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
